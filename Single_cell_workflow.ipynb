{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "import scipy\n",
    "import json\n",
    "import plotly.io as pio\n",
    "from plotly.offline import iplot\n",
    "import plotly as py\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import scipy.stats\n",
    "import math\n",
    "from select import select\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "#only used for the app\n",
    "\n",
    "# import django\n",
    "# from django.conf import settings\n",
    "# from django.contrib.auth.decorators import login_required, permission_required\n",
    "# from file_manager.models import DataAnalysisQueue, SampleRecord, \\\n",
    "#     SavedVisualization, VisualizationApp, UserSettings, ProcessingApp\n",
    "# from django.shortcuts import render\n",
    "# from django.conf import settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03288a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS_FILE = \"settings_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants \n",
    "saved_settings ={}\n",
    "plot_options = {}\n",
    "JUPYTER_MODE = \"JPY_PARENT_PID\" in os.environ #check if it's in jupiter notebook mode\n",
    "APPFOLDER = \"./\"\n",
    "url_base = None\n",
    "\n",
    "#settings\n",
    "WRITE_OUTPUT = False\n",
    "USE_MaxLFQ = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is only for the webapp, not for jupyter notebook\n",
    "def get_run_name(queue_id):\n",
    "    \"\"\"_Get the run name/sample list from the result files, only\n",
    "    used for webapp for populate dropdown list_\n",
    "    Args:\n",
    "        queue_id (_int_): _task id from the process queue_\n",
    "    Returns:\n",
    "        _type_: _pandas data serial contains experiment list_\n",
    "        0              sample1\n",
    "        1              sample2\n",
    "        2              sample3\n",
    "    \"\"\"\n",
    "    if not queue_id:\n",
    "        return None\n",
    "    # get processing name\n",
    "    process_app = DataAnalysisQueue.objects.filter(\n",
    "        pk=queue_id).first().processing_app.name\n",
    "    # fragpipe results\n",
    "    if \"FragPipe\" in process_app:\n",
    "        peptide_file = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2\n",
    "        peptide = pd.read_table(peptide_file)\n",
    "        #\n",
    "        # get experiment names from columns names containning \" MaxLFQ Intensity\"\n",
    "        run_metadata = [\n",
    "            col for col in peptide.columns if \" MaxLFQ Intensity\" in col]\n",
    "        # remove \" MaxLFQ Intensity\" from the experiment names\n",
    "        run_metadata = [name.replace(\" MaxLFQ Intensity\", \"\")\n",
    "                            for name in run_metadata]\n",
    "        # create a pandas series to store the experiment names\n",
    "        run_metadata = pd.Series(run_metadata)\n",
    "    elif \"PD\" in process_app:\n",
    "        inpufile_6 = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_6\n",
    "        meta_table = pd.read_table(inpufile_6)\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        run_metadata =meta_table['file_names']\n",
    "\n",
    "    else:\n",
    "        run_metadata = pd.Series()\n",
    "\n",
    "    return run_metadata\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf24402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def queue_info_api(queue_id, server_address, user_name, password):\n",
    "    \"\"\"_Get the queue and app info from the server through API, this\n",
    "    is only for the jupyter notebook, not for the webapp_\n",
    "    \"\"\"\n",
    "\n",
    "    authinfo = HTTPBasicAuth(user_name, password)\n",
    "\n",
    "    #get queue info and test if the user name and password are correct   \n",
    "\n",
    "    queue_response = requests.get(\n",
    "        f'http://{server_address}/files/api/DataAnalysisQueue/{queue_id}/',\n",
    "        auth=authinfo\n",
    "    )\n",
    "    if queue_response.status_code != 200:\n",
    "        raise Exception(\"Invalid username or password\")\n",
    "    else:\n",
    "        queue_json_data = queue_response.json()\n",
    "\n",
    "    # Get app information\n",
    "    app_response = requests.get(\n",
    "        f\"http://{server_address}/files/api/ProcessingApp/{queue_json_data['processing_app']}/\",\n",
    "        auth=authinfo\n",
    "    )\n",
    "    app_json_data = app_response.json()\n",
    "    return queue_json_data, app_json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(queue_ids = None, queue_info = None, processor_info = None, grouped_input_files = []):\n",
    "    '''\n",
    "    Creates a list of data objects\n",
    "\n",
    "    grouped_input_files\n",
    "    {input1:\n",
    "    input2:\n",
    "    input3:\n",
    "    input4:\n",
    "    input5:\n",
    "    process_app:\n",
    "    }\n",
    "    '''\n",
    "\n",
    "\n",
    "    data_objects = []\n",
    "\n",
    "    i = 0\n",
    "    for eachGroup in grouped_input_files:\n",
    "        if queue_ids is not None:\n",
    "            pass\n",
    "        else:\n",
    "            #print(eachGroup)\n",
    "            process_app = eachGroup[\"process_app\"]\n",
    "            input1= eachGroup[\"input1\"]\n",
    "            input2= eachGroup[\"input2\"]  \n",
    "            input3= eachGroup[\"input3\"]\n",
    "            input4= eachGroup[\"input4\"]  \n",
    "            input5= eachGroup[\"input5\"]\n",
    "\n",
    "        current_data_object = read_file(input1=input1,input2=input2,\n",
    "                                        input3=input3,input4 = input4,\n",
    "                                        input5=input5, process_app=process_app,file_id = i)\n",
    "        data_objects.append(current_data_object)\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    \n",
    "    return data_objects\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(queue_id=None, queue_info= None, processor_info = None,\n",
    "               input1=None, input2=None,input3=None, input4=None, input5=None,\n",
    "            process_app = None, file_id = 1):\n",
    "    \"\"\"_Read data from data manager API or through local files or read directly\n",
    "    in the webapp_\n",
    "    Args:\n",
    "        queue_id (_int_): _processing queue id_\n",
    "        queue_info (_dict_): _queue info from the API_\n",
    "        processor_info (_dict_): _processor info from the API_        \n",
    "        input1 (_str_): _input file 1_\n",
    "        input2 (_str_): _input file 2_\n",
    "        input3 (_str_): _input file 3_\n",
    "        input4 (_str_): _input file 4_\n",
    "        input5 (_str_): _input file 5_\n",
    "        process_app (_str_): _process app name_\n",
    "    Returns:\n",
    "        _dict_: _dictionary containing data all data        \n",
    "    \"\"\"\n",
    "\n",
    "    min_unique_peptides = 1\n",
    "\n",
    "    #getting files from data system\n",
    "        # getting files from data system (webapp)\n",
    "    if queue_id is not None and processor_info is None:\n",
    "        # Method 1 pull data directly (used by the webapp)\n",
    "        process_app = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().processing_app.name\n",
    "        input1= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_1\n",
    "        input2= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2  \n",
    "        input3= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_3\n",
    "        input4= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_4  \n",
    "        input5= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_5\n",
    "    elif queue_info is not None and processor_info is not None:\n",
    "    # Method 2 pull data from the data system (used by jupyter notebook)\n",
    "        process_app = processor_info[\"name\"]\n",
    "        input1= queue_info[\"output_file_1\"]\n",
    "        input2= queue_info[\"output_file_2\"]  \n",
    "        input3= queue_info[\"output_file_3\"]\n",
    "        input4= queue_info[\"output_file_4\"]  \n",
    "        input5= queue_info[\"output_file_5\"]\n",
    "    # method 3 feed data directly (through local file paths)\n",
    "    else:\n",
    "        analysis_file = input1\n",
    "\n",
    "    if \"FragPipe\" in process_app:     # fragpipe results\n",
    "        # read data\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "\n",
    "        # filter Contaminant\n",
    "        peptide_table= peptide_table[~peptide_table[\n",
    "            'Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        protein_table= protein_table[~protein_table['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)].query(\n",
    "            \"`Combined Total Peptides` >= @min_unique_peptides\")\n",
    "        \n",
    "        # get experiment names from columns names containning \"Intensity\"\n",
    "        # or \" MaxLFQ Intensity\" if MaxLFQ is used\n",
    "\n",
    "        if USE_MaxLFQ:\n",
    "            column_tail = \" MaxLFQ Intensity\"\n",
    "            intensity_cols = protein_table.columns[\n",
    "                protein_table.columns.str.contains(column_tail)].tolist()\n",
    "        else:\n",
    "            column_tail = \" Intensity\"\n",
    "            cols = protein_table.columns\n",
    "            intensity_cols =  protein_table.columns[( \\\n",
    "                cols.str.contains(\" Intensity\")) & \\\n",
    "                (~cols.str.contains(\" MaxLFQ Intensity\"))]\n",
    "        #get the column names of protein_table that contain \"Intensity\" but not \" MaxLFQ\"\n",
    "        \n",
    "        ## Proteins abundance table\n",
    "        protein_table.rename(columns={'Protein ID': 'Accession'},inplace=True)\n",
    "\n",
    "        all_path_cols = intensity_cols.append(pd.Index(['Accession']))\n",
    "\n",
    "        prot_abundance = protein_table.loc[:, all_path_cols]\n",
    "\n",
    "\n",
    "        ## Peptide abundance table\n",
    "        peptide_table.rename(columns={'Peptide Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "\n",
    "        all_path_cols = intensity_cols.append(pd.Index(['Annotated Sequence']))\n",
    "\n",
    "\n",
    "\n",
    "        pep_abundance = peptide_table.loc[:, all_path_cols]\n",
    "\n",
    "\n",
    "        # remove \" MaxLFQ Intensity\" or \" Intensity\" from names\n",
    "        run_name_list = [name.replace(column_tail, \"\")\n",
    "                            for name in intensity_cols]\n",
    "        \n",
    "        run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "        prot_abundance = prot_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              prot_abundance.columns if column_tail in col})\n",
    "\n",
    "        pep_abundance = pep_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              pep_abundance.columns if column_tail in col})\n",
    "\n",
    "        prot_abundance = prot_abundance.rename(dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "        pep_abundance = pep_abundance.rename(dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "        # get ID matrix tables\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Annotated Sequence\t']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        prot_other_info = protein_table.loc[\n",
    "            :, ~protein_table.columns.str.contains('Intensity')]\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info = peptide_table.loc[\n",
    "            :, ~peptide_table.columns.str.contains('Intensity')]\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "\n",
    "    elif \"DIANN\" in process_app:\n",
    "        # read in DIANN output files\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        prot_other_info = pd.read_table(input3,low_memory=False)\n",
    "        pep_other_info = pd.read_table(input4,low_memory=False)\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = input3\n",
    "        pep_other_info[\"Source_File\"] = input4\n",
    "\n",
    "        meta_table = pd.read_csv(input5, sep=' ', header=None, names=[\"File Name\"])\n",
    "        meta_table.reset_index(drop=True, inplace=True)\n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[~protein_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        peptide_table= peptide_table[~peptide_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        prot_other_info= prot_other_info[~prot_other_info['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        pep_other_info= pep_other_info[~pep_other_info['Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        \n",
    "        prot_other_info.rename(columns={'Protein': 'Accession'}, inplace=True)\n",
    "        pep_other_info.rename(columns={'Modified.Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "        # Replace backslashes with forward slashes if data comes from Windows\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['File Name'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        run_name_list = meta_table['File Name'].tolist()\n",
    "        run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "\n",
    "        # Get the file names from the meta table\n",
    "        protein_path_cols = protein_table.filter(regex='\\\\\\\\|Protein.Ids').columns\n",
    "\n",
    "        ## Proteins\n",
    "        prot_abundance = protein_table.loc[:, protein_path_cols]\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = protein_table.filter(regex='\\\\\\\\').columns\n",
    "        prot_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in prot_abundance.columns]\n",
    "        prot_abundance = prot_abundance.rename(columns={'Protein.Ids': 'Accession'})\n",
    "        prot_abundance = prot_abundance.rename(columns = dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "        #convert to str for IDs matrix\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        ## Peptides\n",
    "        peptide_path_cols = peptide_table.filter(regex='\\\\\\\\|Modified.Sequence').columns\n",
    "\n",
    "        pep_abundance = peptide_table.loc[:, peptide_path_cols]\n",
    "        pep_abundance = pep_abundance.rename(dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = peptide_table.filter(regex='\\\\\\\\').columns\n",
    "        pep_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in pep_abundance.columns]\n",
    "        pep_abundance = pep_abundance.rename(columns={'Modified.Sequence': 'Annotated Sequence'})\n",
    "\n",
    "        #convert to str for IDs matrix\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "     \n",
    "    elif \"PD\" in process_app:\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        \n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[(protein_table[\n",
    "            \"Protein FDR Confidence: Combined\"] == \"High\") &\n",
    "                        ((protein_table[\"Master\"] == \"IsMasterProtein\") | \n",
    "                         (protein_table[\"Master\"] == \"Master\")) & \n",
    "                        (protein_table[\"Contaminant\"] == False)]\n",
    "\n",
    "        protein_table.rename(\n",
    "            columns={'# Peptides': 'number of peptides'}, inplace=True)\n",
    "        protein_table=protein_table.query(\n",
    "            \"`number of peptides` >= @min_unique_peptides\")\n",
    "        peptide_table= peptide_table[(peptide_table[\n",
    "            'Contaminant'] == False) & (peptide_table[\"Confidence\"]== \"High\")]\n",
    "\n",
    "        meta_table = pd.read_table(input5,low_memory=False)\n",
    "        #filter rows in meta table on File ID column if it is NaN\n",
    "        meta_table = meta_table[meta_table['File ID'].notna()]\n",
    "\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        file_path_name_dict = dict(zip(meta_table['File ID'], meta_table['file_names']))\n",
    "        run_name_list = pd.DataFrame({\"Run Names\": file_path_name_dict.values()})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "        \n",
    "        #format the read in table into three different tables: abundance, id and other_info\n",
    "        prot_abundance = protein_table.filter(regex='Abundance:|Accession')\n",
    "        prot_ID = protein_table.filter(regex='Found in Sample:|Accession')\n",
    "        prot_other_info = protein_table.loc[:, ~protein_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "        \n",
    "\n",
    "        pep_abundance = peptide_table.filter(regex='Abundance:|Annotated Sequence')\n",
    "        pep_ID = peptide_table.filter(regex='Found in Sample:|Annotated Sequence')\n",
    "        pep_other_info = peptide_table.loc[:, ~peptide_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "        #change column names to file/run names to our fileID\n",
    "\n",
    "        new_dict = {\"Abundance: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [prot_abundance,pep_abundance]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "            #TODO solving  A value is trying to be set on a copy of a slice from a DataFrame\n",
    "            \n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            fileid_mapping = generate_column_from_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        new_dict = {\"Found in Sample: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [pep_ID,prot_ID]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "\n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            fileid_mapping = generate_column_from_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "\n",
    "        # replace \"High\" to MS2 \"Peak Found\" to MBR, the rest to NaN for the ID tables\n",
    "        replacements = {'High': 'MS2', 'Peak Found': 'MBR'}\n",
    "        for column in run_name_list[\"Run Identifier\"]:\n",
    "            if column in pep_ID.columns:\n",
    "                pep_ID[column] = pep_ID[column].apply(custom_replace)\n",
    "    \n",
    "            if column in prot_ID.columns:\n",
    "                prot_ID[column] = prot_ID[column].apply(custom_replace)\n",
    "    \n",
    "    # get ID summary, tolly\n",
    "    protein_ID_summary = sumIDs(prot_ID)\n",
    "    peptide_ID_summary = sumIDs(pep_ID)\n",
    "\n",
    "\n",
    "    \n",
    "#     peptide_intensities = AbundanceMatrix(\n",
    "#         peptide_table, maxLFQ_intensity=False, isProtein=False)\n",
    "#     peptide_ID_matrix = toIDMatrix(peptide_intensities)\n",
    "#     peptide_ID_summary = sumIDs(peptide_ID_matrix)\n",
    "\n",
    "    #sets the processing app in run_name_list\n",
    "    run_name_list[\"Processing App\"] = process_app\n",
    "    run_name_list[\"Analysis Name\"] = analysis_file\n",
    "\n",
    "\n",
    "    return {'run_metadata': run_name_list,\n",
    "            'protein_other_info': prot_other_info,\n",
    "            'peptide_other_info': pep_other_info,\n",
    "            'protein_abundance': prot_abundance,\n",
    "            'protein_ID_matrix': prot_ID,\n",
    "            'protein_ID_Summary': protein_ID_summary,\n",
    "            'peptide_abundance': pep_abundance,\n",
    "            'peptide_ID_matrix': pep_ID,\n",
    "            'peptide_ID_Summary': peptide_ID_summary,\n",
    "\n",
    "            }  \n",
    "\n",
    "# Define a custom replace function\n",
    "def custom_replace(value):\n",
    "    if value == 'High':\n",
    "        return 'MS2'\n",
    "    elif value == 'Peak Found':\n",
    "        return 'MBR'\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# Function to generate a new column name mapping based on the partial_column_name_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_from_name_mapping(columns, partial_column_name_mapping):\n",
    "    column_name_mapping = {}\n",
    "    print(partial_column_name_mapping)\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key in col:\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_to_name_mapping(columns, partial_column_name_mapping):\n",
    "    column_name_mapping = {}\n",
    "    #print(partial_column_name_mapping)\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key == col:\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumIDs(IDMatrix):\n",
    "    \"\"\"_summarize the ID matrix infor into ID summary_\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        IDMatrix (_type_): _protein or pepetides matrix_\n",
    "        0 Accession/Annotated Sequence \trun1 \trun2 \trun3 \n",
    "        1 P023D12\tMS2 \tMBR \tNaN \n",
    "        2 P1222\tNaN \tID \tNaN \n",
    "    ID: means we don't know the ID mode\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "                                      names  MS2_IDs  MBR_IDs  Total_IDs\n",
    "0            10ng_QC_1_channel2 Intensity      NaN      NaN       3650\n",
    "1            10ng_QC_2_channel1 Intensity      NaN      NaN       3604\n",
    "....\n",
    "    \"\"\"\n",
    "    # Select the columns of the runs\n",
    "    columns = [col for col in IDMatrix.columns if not any(\n",
    "        substring in col for substring in [\n",
    "            'Accession', 'Annotated Sequence'])]\n",
    "    #put each ID types into a list\n",
    "    returnNames = []\n",
    "    MS2_ID = []\n",
    "    MBR_ID = []\n",
    "    total_ID = []\n",
    "    for eachColumn in columns:\n",
    "        MS2_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MS2\"]))\n",
    "        MBR_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MBR\"]))\n",
    "        print(IDMatrix[eachColumn])\n",
    "        total_ID_each = len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"ID\"])\n",
    "        if total_ID_each ==0:\n",
    "            total_ID_each = len(IDMatrix[eachColumn][\n",
    "                IDMatrix[eachColumn] == \"MS2\"]) + len(IDMatrix[\n",
    "                eachColumn][IDMatrix[eachColumn] == \"MBR\"])\n",
    "        total_ID.append(total_ID_each)\n",
    "\n",
    "    return pd.DataFrame({'names': columns,\n",
    "                         'MS2_IDs': MS2_ID,\n",
    "                         'MBR_IDs': MBR_ID,\n",
    "                         'Total_IDs': total_ID})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_join_data_objects(data_objects):\n",
    "    '''\n",
    "    Takes in a list of data objects as given by read_files and converts them to a single data object as given by read_files,\n",
    "    protein info continues to show what was found on each original file, and so forth.\n",
    "    '''\n",
    "\n",
    "    first_file = True\n",
    "    for eachDataObject in data_objects:\n",
    "        print(\"***\")\n",
    "        if first_file:\n",
    "            first_file = False\n",
    "            final_data_object = eachDataObject\n",
    "        else:\n",
    "            final_data_object['run_metadata'] = pd.concat([final_data_object['run_metadata'],eachDataObject['run_metadata']]).reset_index()\n",
    "            final_data_object['protein_other_info'] = pd.concat([final_data_object['protein_other_info'],eachDataObject['protein_other_info']]).reset_index()\n",
    "            final_data_object['peptide_other_info'] = pd.concat([final_data_object['peptide_other_info'],eachDataObject['peptide_other_info']]).reset_index()\n",
    "            final_data_object['protein_ID_Summary'] = pd.concat([final_data_object['protein_ID_Summary'],eachDataObject['protein_ID_Summary']]).reset_index()\n",
    "            final_data_object['peptide_ID_Summary'] = pd.concat([final_data_object['peptide_ID_Summary'],eachDataObject['peptide_ID_Summary']]).reset_index()\n",
    "            duplicates_found = False\n",
    "            \n",
    "            #loop through to see if there are any duplicate files\n",
    "            for eachCol in final_data_object['protein_abundance'].loc[:, final_data_object['protein_abundance'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['protein_ID_matrix'].loc[:, final_data_object['protein_ID_matrix'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_abundance'].loc[:, final_data_object['peptide_abundance'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_ID_matrix'].loc[:, final_data_object['peptide_ID_matrix'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass     \n",
    "            if duplicates_found:\n",
    "                print(\"Error: files analyzed twice present!!!\")\n",
    "                quit()\n",
    "                print(\"@#afio2q3\")\n",
    "            else:\n",
    "                #merge keeping all proteins\n",
    "                print(\"!!!!\")\n",
    "                final_data_object['protein_abundance'] = pd.merge(final_data_object['protein_abundance'],eachDataObject['protein_abundance'],how=\"outer\")\n",
    "                final_data_object['protein_ID_matrix'] = pd.merge(final_data_object['protein_ID_matrix'],eachDataObject['protein_ID_matrix'],how=\"outer\")\n",
    "                final_data_object['peptide_abundance'] = pd.merge(final_data_object['peptide_abundance'],eachDataObject['peptide_abundance'],how=\"outer\")\n",
    "                final_data_object['peptide_ID_matrix'] = pd.merge(final_data_object['peptide_ID_matrix'],eachDataObject['peptide_ID_matrix'],how=\"outer\")\n",
    "                \n",
    "    return final_data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_missing_values(data_object,\n",
    "                             missing_value_thresh=33,\n",
    "                             is_protein=True,\n",
    "                             ignore_nan=False):\n",
    "    \"\"\"_Filter out proteins/peptides with missing values rate above the\n",
    "    threshold_\n",
    "\n",
    "    Args:\n",
    "        data_object (_panada_): _dataframe contain data for one experimental\n",
    "        condition_\n",
    "        missing_value_thresh (int, optional): _description_. Defaults to 33.\n",
    "        analysis_program (str, optional): _description_.\n",
    "        ignore_nan: if filter intensity again with Nan threadshold, this \n",
    "        helps with the calcualting stdev step.\n",
    "\n",
    "    Returns:\n",
    "        _data_object_: _dictionary containing data for one experimental\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "0     A0A096LP49                            0.00\n",
    "1     A0A0B4J2D5                        89850.26\n",
    "2         A0AVT1                        83055.87\n",
    "    \"\"\"\n",
    "    if is_protein:\n",
    "        name = \"Accession\"\n",
    "        matrix_name = \"protein_ID_matrix\"\n",
    "        other_info_name = \"protein_other_info\"\n",
    "        abundance_name = \"protein_abundance\"\n",
    "        \n",
    "    else:\n",
    "        name = \"Annotated Sequence\"\n",
    "        matrix_name = \"peptide_ID_matrix\"\n",
    "        other_info_name = \"peptide_other_info\"\n",
    "        abundance_name = \"peptide_abundance\"\n",
    "\n",
    "    protein_columns = data_object[matrix_name].assign(missingValues=0)\n",
    "\n",
    "    i = 0\n",
    "    # found all the proteins/peptides with missing values rate below\n",
    "    # the threshold, pep_columns contains the remaining protein/peptide\n",
    "    # in a pandas dataframe with $names as its column name\n",
    "    for each_column in data_object[matrix_name].loc[\n",
    "            :, ~data_object[matrix_name].columns.str.contains(\n",
    "                name)].columns:\n",
    "        # replace \"nan\" to np.nan\n",
    "        protein_columns = protein_columns.replace({\"nan\": np.nan}) \n",
    "        protein_columns.loc[protein_columns[each_column].isnull(),\n",
    "                             \"missingValues\"] += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    protein_columns = protein_columns.assign(missingValuesRate=(\n",
    "        protein_columns[\"missingValues\"] / i) * 100)\n",
    "    \n",
    "    protein_columns = protein_columns.query(\n",
    "        \"missingValuesRate < @missing_value_thresh\")\n",
    "    \n",
    "    protein_columns = protein_columns.loc[:,\n",
    "                                  protein_columns.columns.str.contains(name)]\n",
    "\n",
    "    # filter the data_object with the remaining proteins/peptides names\n",
    "    data_object[abundance_name] = protein_columns.merge(\n",
    "        data_object[abundance_name])\n",
    "    data_object[matrix_name] = protein_columns.merge(\n",
    "        data_object[matrix_name])\n",
    "    data_object[other_info_name] = protein_columns.merge(\n",
    "        data_object[other_info_name])\n",
    "    # In case there is mismatch between ID table and abundance table,\n",
    "    # mannually remove the row with all NaN values\n",
    "    # keep rows in data_object[abundance_name] where at least two values are \n",
    "    # not NaN(do this to all rows except the first row), otherwise can't\n",
    "    # calculate the stdev\n",
    "    if ignore_nan:\n",
    "        data_object[abundance_name] = data_object[abundance_name].dropna(\n",
    "            thresh=2, subset=data_object[abundance_name].columns[1:])\n",
    "        # This will cause the veen diagram to be different from R program\n",
    "    \n",
    "    return data_object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeToMedian(abundance_data, apply_log2=False):\n",
    "    \"\"\"_Normalizes each column by multiplying each value in that column with\n",
    "    the median of all values in abundances (all experiments) and then dividing\n",
    "    by the median of that column (experiment)._\n",
    "    Args:\n",
    "        abundance_data (_pd_): _description_\n",
    "        apply_log2 (_bool_,): _apply log2 to all result_.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        format:\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "         A0A096LP49                    0.000000e+00\n",
    "    \"\"\"\n",
    "    # all the columns/sample list\n",
    "    columns = [col for col in abundance_data.select_dtypes(include=[\n",
    "            np.number])]\n",
    "    data_matrix = abundance_data[columns].values\n",
    "    # replace 0 with nan\n",
    "    data_matrix[data_matrix == 0] = np.nan\n",
    "    medianOfAll = np.nanmedian(data_matrix)\n",
    "    \n",
    "    #normalize all median, all median/current run all protein median\n",
    "    # apply log2 to all the values if apply_log2 is True\n",
    "    if apply_log2:    \n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                np.log2(medianOfAll) * np.log2(abundance_data[each_column]) /\n",
    "                np.log2(np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan))))\n",
    "    else:\n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                medianOfAll * abundance_data[each_column] /\n",
    "                np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan)))\n",
    "    #TODO divide by zero error encountered in log2, temporarily set to 0\n",
    "    abundance_data = abundance_data.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cvs(abundance_data):\n",
    "    \"\"\"_Calculate mean, stdev, cv for withn each protein/peptide abundance_\n",
    "\n",
    "    Args:\n",
    "        data_object (_type_): _full data frame_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _df with Accession mean, stdev, cv for each protein/peptide_\n",
    "    \"\"\"\n",
    "    if 'Accession' in abundance_data.columns:\n",
    "        name = \"Accession\"\n",
    "    if 'Annotated Sequence' in abundance_data.columns:\n",
    "        name = \"Annotated Sequence\"\n",
    "    abundance_data = abundance_data.assign(\n",
    "        intensity=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].mean(axis=1, skipna=True),\n",
    "        stdev=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].std(axis=1, skipna=True),\n",
    "        CV=abundance_data.loc[:, ~abundance_data.columns.str.contains(name)].std(\n",
    "            axis=1, skipna=True) / abundance_data.loc[\n",
    "            :, ~abundance_data.columns.str.contains(name)].mean(\n",
    "            axis=1, skipna=True) * 100)\n",
    "\n",
    "    abundance_data = abundance_data.loc[:, [\n",
    "            name, \"intensity\", \"stdev\", \"CV\"]]\n",
    "    \n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_from_summary_stats(m1, m2, n1, n2, s1, s2, equal_var=False):\n",
    "    \"\"\"_Calculate T-test from summary using ttest_ind_from_stats from\n",
    "    scipy.stats package_\n",
    "\n",
    "    Args:\n",
    "        m1 (_type_): _mean list of sample 1_\n",
    "        m2 (_type_): mean list of sample 2_\n",
    "        n1 (_type_): sample size list of sample 1_\n",
    "        n2 (_type_): sample size list of sample 2_\n",
    "        s1 (_type_): standard deviation list of sample 1_\n",
    "        s2 (_type_): standard deviation list of sample 2_\n",
    "        equal_var (_type_, optional): False would perform Welch's\n",
    "        t-test, while set it to True would perform Student's t-test. Defaults\n",
    "        to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _list of P values_\n",
    "    \"\"\"\n",
    "\n",
    "    p_values = []\n",
    "    for i in range(len(m1)):\n",
    "        _, p = ttest_ind_from_stats(\n",
    "            m1[i], s1[i], n1[i], m2[i], s2[i], n2[i], equal_var=equal_var)\n",
    "        p_values.append(p)\n",
    "\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineSharedInformation(infoObject1, infoObject2):\n",
    "    \"\"\"_Combine two infoObject into one_\n",
    "    #TODO: there must be a better way to achieve this.\n",
    "    \"\"\"\n",
    "\n",
    "    if infoObject1['meta'][\"quan_method\"] == 'Protein':\n",
    "        name = 'Accession'\n",
    "    else:\n",
    "        name = 'Annotated Sequence'\n",
    "\n",
    "    infoObject = {'meta': None,\n",
    "                  'run_name': None,\n",
    "                  'protein_data': None,\n",
    "                  'abundances': None,\n",
    "                  'protein_ID_matrix': None,\n",
    "                  'protein_ID_Summary': None}\n",
    "\n",
    "    if infoObject1['meta'] == infoObject2['meta']:\n",
    "        infoObject['meta'] = infoObject1['meta']\n",
    "        infoObject['run_name'] = pd.concat(\n",
    "            [infoObject1['run_name'], infoObject2['run_name']], axis=0)\n",
    "        infoObject['protein_ID_Summary'] = pd.concat(\n",
    "            [infoObject1['protein_ID_Summary'], infoObject2[\n",
    "                'protein_ID_Summary']], axis=0)\n",
    "        infoObject['protein_ID_matrix'] = pd.merge(\n",
    "            infoObject1['protein_ID_matrix'], infoObject2[\n",
    "                'protein_ID_matrix'], on=name)\n",
    "        infoObject = pd.merge(\n",
    "            infoObject1, infoObject2, on=name)\n",
    "        infoObject['protein_data'] = pd.merge(infoObject1[\n",
    "            'protein_data'].loc[:, infoObject1[\n",
    "                'protein_data'].columns.str.contains(name)],\n",
    "            infoObject2['protein_data'],\n",
    "            on=name)\n",
    "    else:\n",
    "        infoObject = \"ERROR: incompatible data types\"\n",
    "\n",
    "    return infoObject\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(abundance_data, k=5):\n",
    "    \"\"\"_inpute missing value from neighbor values_\n",
    "\n",
    "    Args:\n",
    "        abundance_data (_type_): _description_\n",
    "        k (int, optional): _number of neighbors used_. Defaults to 5.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        TODO: this knn imputer produces slightly different results (about 4%)\n",
    "        from the one in R. Need to figure out why\n",
    "    \"\"\"\n",
    "    name = abundance_data.columns[0]\n",
    "\n",
    "    names = abundance_data[name]\n",
    "    # x = abundance_data.select_dtypes(include=['float64', 'int64'])\n",
    "    # imputer = KNNImputer(n_neighbors=k)\n",
    "    # x_imputed = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)\n",
    "\n",
    "\n",
    "    x = abundance_data.select_dtypes(include=['float', 'int'])\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    x_imputed = imputer.fit_transform(x)\n",
    "    x_imputed = pd.DataFrame(x_imputed, columns=x.columns)\n",
    "\n",
    "\n",
    "\n",
    "    abundance_data.loc[:, x.columns] = x_imputed.values\n",
    "    abundance_data[name] = names\n",
    "    return abundance_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculatePCA(abundance_object, infotib,log2T = False):\n",
    "    \"\"\"_inpute PCA transformed and variance explained by each principal\n",
    "    component_\n",
    "    \"\"\"\n",
    "    name = abundance_object.columns[0]\n",
    "    x = abundance_object\n",
    "\n",
    "    sampleNames = x.columns[~x.columns.str.contains(\n",
    "        name)].to_frame(index=False)\n",
    "\n",
    "    if log2T: #apply log2 transformation\n",
    "        x = np.log2(x.loc[:, ~x.columns.str.contains(name)].T.values)\n",
    "    else:\n",
    "        x = x.loc[:, ~x.columns.str.contains(name)].T.values\n",
    "    # filter out columns with all zeros\n",
    "    is_finite_col = np.isfinite(np.sum(x, axis=0))\n",
    "    x_filtered = x[:, is_finite_col]\n",
    "\n",
    "    \n",
    "    # Instantiate PCA    \n",
    "    pca = PCA()\n",
    "    #\n",
    "    # Determine transformed features\n",
    "    #\n",
    "    x_pca = pca.fit_transform(x_filtered)\n",
    "    #\n",
    "    # Determine explained variance using explained_variance_ration_ attribute\n",
    "    #\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "    #\n",
    "    # Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "    # for visualizing the variance explained by each principal component.\n",
    "    #\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "    #\n",
    "    # convert numpy array to pandas dataframe for plotting\n",
    "    \n",
    "    pca_panda = pd.DataFrame(x_pca, columns=[\n",
    "        'PC' + str(i+1) for i in range(x_pca.shape[1])])\n",
    "    # add sample names to the dataframe\n",
    "    pca_panda = pd.concat(\n",
    "        [infotib, pca_panda], axis=1, join='inner')\n",
    "    \n",
    "    return pca_panda, exp_var_pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(data_dict, runname_list):\n",
    "    \"\"\"_Filter the data_dict based on runname_list, only keep the columns\n",
    "    of the data_dict that are in the runname_list_\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    filtered_data = {}\n",
    "   # filtered_data[\"meta\"] = data_dict[\"meta\"]\n",
    "    runname_list.extend([\"Annotated Sequence\",\"Accession\"])\n",
    "    \n",
    "    # filter in good ones\n",
    "    filtered_data[\"run_metadata\"] = [item for item in data_dict[\n",
    "        \"run_metadata\"] if item in runname_list]  \n",
    "    filtered_data[\"protein_abundance\"] = data_dict[\"protein_abundance\"][[\n",
    "        col for col in data_dict[\"protein_abundance\"].columns if any(\n",
    "            word == col for word in runname_list)]]\n",
    "    filtered_data[\"peptide_abundance\"] = data_dict[\"peptide_abundance\"][[\n",
    "        col for col in data_dict[\"peptide_abundance\"].columns if any(\n",
    "            word == col for word in runname_list)]]\n",
    "    filtered_data[\"protein_other_info\"] = data_dict[\"protein_other_info\"][[\n",
    "        col for col in data_dict[\"protein_other_info\"].columns if any(\n",
    "            word == col for word in runname_list)]]\n",
    "    filtered_data[\"peptide_other_info\"] = data_dict[\"peptide_other_info\"][[\n",
    "        col for col in data_dict[\"peptide_other_info\"].columns if any(\n",
    "            word == col for word in runname_list)]]\n",
    "    filtered_data[\"protein_ID_matrix\"] = data_dict[\"protein_ID_matrix\"][[\n",
    "        col for col in data_dict[\"protein_ID_matrix\"].columns if any(\n",
    "            word == col for word in runname_list)]]\n",
    "    filtered_data[\"protein_ID_Summary\"] = data_dict[\"protein_ID_Summary\"][\n",
    "        data_dict[\"protein_ID_Summary\"][\"names\"].isin(\n",
    "            runname_list)]\n",
    "    filtered_data[\"peptide_ID_Summary\"] = data_dict[\"peptide_ID_Summary\"][\n",
    "        data_dict[\"peptide_ID_Summary\"][\"names\"].isin(\n",
    "            runname_list)]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_id_Kei(data_dict, run_ids):\n",
    "    filtered_data = {}\n",
    "    print(run_ids)\n",
    "    display(data_dict[\"run_metadata\"])\n",
    "    filtered_data[\"run_metadata\"] = data_dict[\"run_metadata\"][data_dict[\"run_metadata\"][\"Run Identifier\"].isin(run_ids)]\n",
    "\n",
    "    \n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein peptide identification bar\n",
    "    plot_\n",
    "\n",
    "    Args:\n",
    "        data_dict (_type_): _description_\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to store the group names and filters\n",
    "    group_names = saved_settings.keys()\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            runname_sublist)  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "        print(group_dict[eachGroup][\"run_metadata\"])\n",
    "\n",
    "    # create ID plots\n",
    "    # allIDs table will be used to store all experiment name, ID types (\n",
    "    # protein, peptide, MS2 and MS1 based), conditions and IDs numbers\n",
    "    allIDs = pd.DataFrame(\n",
    "        columns=[\"Names\", \"ID_Type\", \"ID_Mode\", \"Conditions\", \"IDs\"])\n",
    "\n",
    "    # loop through each group and extract IDs, put them into allIDs table\n",
    "    for eachCondition in group_names:\n",
    "        # Protein ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"protein_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"protein_ID_Summary\"].at[index, item]):\n",
    "                    # if the row with the item column is not empty,\n",
    "                    # add it to allIDs table.\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"protein\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "        # Peptide ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"peptide_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"peptide_ID_Summary\"].at[index, item]):\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"peptide\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "    # ######################allIDs format###################\n",
    "    # name\tID_Type\tID_Mode\tConditions\tIDs\n",
    "    # file1\tpeptide\tMS2_IDs\texperimetn 1\txxxxx\n",
    "    # file2\tprotein\tMBR_IDs\texperiment 2\txxxx\n",
    "    # file3\tpeptide\tTotal_IDs\texperiment 3\txxx\n",
    "    #######################################################\n",
    "    # Calcuate mean, standard deviation and number of replicates for each\n",
    "\n",
    "    # choose protein or peptide\n",
    "    if plot_options[\"plot_type\"] == \"1\":  # Protein ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"protein\"]\n",
    "    elif plot_options[\"plot_type\"] == \"2\":  # Peptide ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"peptide\"]\n",
    "\n",
    "    # choose total, MS2 or stacked\n",
    "    if plot_options[\"ID mode\"] == \"MS2\":  # MS2 ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"MS2_IDs\"]\n",
    "    elif plot_options[\"ID mode\"] == \"stacked\":  # total separated\n",
    "        pass\n",
    "    else:\n",
    "        # total ID combined, if not already summed (key exist), sum them\n",
    "#         if allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"].empty:\n",
    "#             grouped = allIDs.groupby('name').agg(\n",
    "#                 {'IDs': 'sum', 'ID_Type': 'first', 'Conditions': 'first'})\n",
    "#             grouped = grouped.reset_index()\n",
    "#             grouped[\"ID_Mode\"] = \"Total_IDs\"\n",
    "#             allIDs = grouped\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "    toPlotIDs = allIDs.groupby([\"ID_Mode\", \"Conditions\"]).agg({\n",
    "        'IDs': ['mean', 'std', 'count'], 'ID_Type': 'first', })\n",
    "\n",
    "    # rename the columns\n",
    "    toPlotIDs.columns = ['IDs', 'stdev', 'n', 'ID_Type']\n",
    "    # reset the index after grouping\n",
    "    toPlotIDs = toPlotIDs.reset_index()\n",
    "    # calculate the confidence interval based on 95%confidence interval`\n",
    "    toPlotIDs[\"confInt\"] = t.ppf(0.975, toPlotIDs['n']-1) * \\\n",
    "        toPlotIDs['stdev']/np.sqrt(toPlotIDs['n'])\n",
    "\n",
    "    return plot_IDChart_plotly(toPlotIDs,\n",
    "                               username=username,\n",
    "                               plot_options=plot_options)\n",
    "def plot_IDChart_plotly(ID_data,\n",
    "                        username=None,\n",
    "                        plot_options=None):\n",
    "    \"\"\"_Plot the ID bar plot for the given data_\n",
    "\n",
    "    Args:\n",
    "        ID_data (_type_): _description_\n",
    "        username (str, optional): _description_. Defaults to \"test\".\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "        \n",
    "    if plot_options[\"ID mode\"] != \"stacked\":\n",
    "\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "        else:\n",
    "            error_bars = None\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "\n",
    "\n",
    "        # create the plot\n",
    "        fig = px.bar(ID_data,\n",
    "                     x=\"Conditions\",\n",
    "                     y=\"IDs\",\n",
    "                     error_y=error_bars,\n",
    "                     color=\"Conditions\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],\n",
    "                     )\n",
    "        fig.update_layout(xaxis_title=plot_options[\"X Title\"],\n",
    "                          yaxis_title=plot_options[\"Y Title\"],\n",
    "                          annotations=total_labels,\n",
    "                          font=plot_options[\"font\"]\n",
    "                          )\n",
    "    if WRITE_OUTPUT:    \n",
    "        # export the data to csv for user downloading\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        # create the directory if it does not exist\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "\n",
    "        # export the data to csv\n",
    "        ID_data.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_ID_data.csv\"), index=False)\n",
    "        # create the link for downloading the data\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_ID_data.csv\"\n",
    "\n",
    "        # add SVG download link\n",
    "\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_Bar_Plot.svg\"\n",
    "\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_Bar_Plot.svg\"))\n",
    "\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    return fig, CSV_link, SVG_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Violin plots ###\n",
    "def CV_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein CV violin plots_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "    for item in GROUP_NAME:\n",
    "        if saved_settings[item]:\n",
    "            group_names.append(saved_settings[item])\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = [d.get('option')\n",
    "                           for d in saved_settings[f\"group{i}_record\"]]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "\n",
    "    for eachGroup in group_names:\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "            current_condition_data[\"protein_abundance\"])\n",
    "\n",
    "    all_cvs = pd.DataFrame()\n",
    "\n",
    "    for eachGroup in Intensity_dict:\n",
    "        current = calculate_cvs(\n",
    "            Intensity_dict[eachGroup]).assign(Conditions=eachGroup)\n",
    "        all_cvs = pd.concat([all_cvs, current], ignore_index=True)\n",
    "\n",
    "    # ######################all_CVs format###################\n",
    "#      Accession     intensity          stdev          CV   Conditions\n",
    "# 0       A6NHR9  3.248547e+06  672989.819300   20.716643    DDMandDTT\n",
    "# 1       A8MTJ3  5.031539e+05  195535.383583   38.861944    DDMandDTT\n",
    "# 2       E9PAV3  5.330290e+05  161385.491163   30.277056    DDMandDT\n",
    "    #######################################################\n",
    "\n",
    "    return plot_CV_violin(allCVs=all_cvs,\n",
    "                          username=username,\n",
    "                          plot_options=plot_options)\n",
    "\n",
    "\n",
    "def plot_CV_violin(allCVs,\n",
    "                   username=None,\n",
    "                   plot_options=None,\n",
    "                   ):\n",
    "    \"\"\"_Plot the CV violin plot for the given data._\n",
    "\n",
    "    Args:\n",
    "        allCVs (_type_): _description_\n",
    "        username (_type_, optional): _description_. Defaults to None.\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    allCVs_summary = allCVs.groupby([\"Conditions\"]).agg(\n",
    "        {'CV': ['median', 'mean']}).reset_index()\n",
    "    allCVs_summary.columns = [\"Conditions\", 'meds', 'CoVar']\n",
    "    # mean label\n",
    "    if plot_options[\"mean label\"] == \"True\" or \\\n",
    "            plot_options[\"mean label\"] == True:\n",
    "        total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "            round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "            allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "    else:\n",
    "        total_labels = []   # no mean labels\n",
    "\n",
    "    \n",
    "    # create the interactive plot\n",
    "    fig = px.violin(allCVs,\n",
    "                    x=\"Conditions\",\n",
    "                    y='CV',\n",
    "                    color=\"Conditions\",\n",
    "                    box=bool(plot_options[\"box\"]),\n",
    "                    violinmode=plot_options[\"violinmode\"], hover_data=[\n",
    "                        \"Conditions\", 'CV'],\n",
    "                    color_discrete_sequence=plot_options[\"color\"],\n",
    "                    width=plot_options[\"width\"],\n",
    "                    height=plot_options[\"height\"],\n",
    "                    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                   range=plot_options[\"ylimits\"]),\n",
    "        font=plot_options[\"font\"],\n",
    "        xaxis=dict(title=plot_options[\"X Title\"]),\n",
    "        showlegend=True,\n",
    "        annotations=total_labels\n",
    "    )\n",
    "\n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_CV_Violin_Plot.svg\"))\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        allCVs.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_CV.csv\"), index=False)\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_CV.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_CV_Violin_Plot.svg\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venns_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating ID veens plots (up to three groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    if not plot_options[\"compare groups\"] or \\\n",
    "            plot_options[\"compare groups\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"compare groups\"], list):\n",
    "        for item in GROUP_NAME[:2]:\n",
    "            if saved_settings[item]:\n",
    "                group_names.append(saved_settings[item])\n",
    "    else:  # compare groups (list of 2/3 numbers, start with 0) is provided,\n",
    "        for n in range(len(GROUP_NAME)):\n",
    "            if n in plot_options[\"compare groups\"] and saved_settings[\n",
    "                    GROUP_NAME[n]]:\n",
    "                group_names.append(saved_settings[GROUP_NAME[n]])\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = [d.get('option')\n",
    "                           for d in saved_settings[\n",
    "            f\"group{plot_options['compare groups'][i]+1}_record\"]]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    data_set = []\n",
    "    labels_set = []\n",
    "    \n",
    "    for eachGroup in group_names:\n",
    "        \n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "\n",
    "        data_set.append(\n",
    "            set(current_condition_data['protein_abundance']['Accession'].unique()))\n",
    "        labels_set.append(eachGroup)\n",
    "\n",
    "    return venn_to_plotly(\n",
    "        data_set,\n",
    "        labels_set,\n",
    "        plot_options=plot_options,\n",
    "        username=username)\n",
    "\n",
    "\n",
    "def venn_to_plotly(L_sets,\n",
    "                   L_labels=None,\n",
    "                   plot_options=None,\n",
    "                   username=None):\n",
    "    \"\"\"_Creates a venn diagramm from a list of\n",
    "    sets and returns a plotly figure_\n",
    "    \"\"\"\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    # get number of sets\n",
    "    n_sets = len(L_sets)\n",
    "\n",
    "    # choose and create matplotlib venn diagramm\n",
    "    if n_sets == 2:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn2(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn2(L_sets)\n",
    "    elif n_sets == 3:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn3(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn3(L_sets)\n",
    "    # supress output of venn diagramm\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Create empty lists to hold shapes and annotations\n",
    "    L_shapes = []\n",
    "    L_annotation = []\n",
    "\n",
    "    # Define color list for sets\n",
    "    L_color = plot_options[\"color\"]\n",
    "\n",
    "    # Create empty list to make hold of min and max values of set shapes\n",
    "    L_x_max = []\n",
    "    L_y_max = []\n",
    "    L_x_min = []\n",
    "    L_y_min = []\n",
    "\n",
    "    for i in range(0, n_sets):\n",
    "\n",
    "        # create circle shape for current set\n",
    "\n",
    "        shape = go.layout.Shape(\n",
    "            type=\"circle\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            x0=v.centers[i][0] - v.radii[i],\n",
    "            y0=v.centers[i][1] - v.radii[i],\n",
    "            x1=v.centers[i][0] + v.radii[i],\n",
    "            y1=v.centers[i][1] + v.radii[i],\n",
    "            fillcolor=L_color[i],\n",
    "            line_color=L_color[i],\n",
    "            opacity=plot_options[\"opacity\"]\n",
    "        )\n",
    "\n",
    "        L_shapes.append(shape)\n",
    "\n",
    "        # create set label for current set\n",
    "        try:\n",
    "            anno_set_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.set_labels[i].get_position()[0],\n",
    "                y=v.set_labels[i].get_position()[1],\n",
    "                text=v.set_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_set_label)\n",
    "\n",
    "            # get min and max values of current set shape\n",
    "            L_x_max.append(v.centers[i][0] + v.radii[i])\n",
    "            L_x_min.append(v.centers[i][0] - v.radii[i])\n",
    "            L_y_max.append(v.centers[i][1] + v.radii[i])\n",
    "            L_y_min.append(v.centers[i][1] - v.radii[i])\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "\n",
    "    # determine number of subsets\n",
    "    n_subsets = sum([scipy.special.binom(n_sets, i+1)\n",
    "                     for i in range(0, n_sets)])\n",
    "\n",
    "    for i in range(0, int(n_subsets)):\n",
    "        try:\n",
    "\n",
    "            # create subset label (number of common elements for current subset\n",
    "\n",
    "            anno_subset_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.subset_labels[i].get_position()[0],\n",
    "                y=v.subset_labels[i].get_position()[1],\n",
    "                text=v.subset_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_subset_label)\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "    # define off_set for the figure range\n",
    "    off_set = 0.2\n",
    "\n",
    "    # get min and max for x and y dimension to set the figure range\n",
    "    x_max = max(L_x_max) + off_set\n",
    "    x_min = min(L_x_min) - off_set\n",
    "    y_max = max(L_y_max) + off_set\n",
    "    y_min = min(L_y_min) - off_set\n",
    "\n",
    "    # create plotly figure\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # set xaxes range and hide ticks and ticklabels\n",
    "    fig.update_xaxes(\n",
    "        range=[x_min, x_max],\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set yaxes range and hide ticks and ticklabels\n",
    "    fig.update_yaxes(\n",
    "        range=[y_min, y_max],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set figure properties and add shapes and annotations\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(b=0, l=10, pad=0, r=10, t=40),\n",
    "        width=800,\n",
    "        height=400,\n",
    "        shapes=L_shapes,\n",
    "        annotations=L_annotation,\n",
    "        title=dict(text=plot_options[\"title\"], x=0.5, xanchor='center')\n",
    "    )\n",
    "    if WRITE_OUTPUT:\n",
    "        # SVG file link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_venns_Plot.svg\"\n",
    "\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_venns_Plot.svg\"))\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Volcano plots####\n",
    "def volcano_plots(data_object,  plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating intensity volcano plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    if not plot_options[\"compare groups\"] or \\\n",
    "            plot_options[\"compare groups\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"compare groups\"], list):\n",
    "        for item in GROUP_NAME[:2]:\n",
    "            if saved_settings[item]:\n",
    "                group_names.append(saved_settings[item])\n",
    "    else:  # compare groups (list of two, start with 0) is provided,\n",
    "        for n in range(len(GROUP_NAME)):\n",
    "            if n in plot_options[\"compare groups\"] and saved_settings[\n",
    "                    GROUP_NAME[n]]:\n",
    "                group_names.append(saved_settings[GROUP_NAME[n]])\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = [d.get('option')\n",
    "                           for d in saved_settings[\n",
    "            f\"group{plot_options['compare groups'][i]+1}_record\"]]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "\n",
    "    for eachGroup in group_names:\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "            current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "    group1 = group_names[0]\n",
    "    group2 = group_names[1]\n",
    "    # calculate mean, standard deviation, and the number of non-null\n",
    "    # elements for each row/protein\n",
    "    group1Data = (Intensity_dict[group1]\n",
    "                  .assign(group1_Intensity=Intensity_dict[group1].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group1_stdev=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group1_num=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).shape[1] - Intensity_dict[\n",
    "                      group1].isna().sum(axis=1))\n",
    "                  .loc[:, ['group1_Intensity',\n",
    "                           'group1_stdev',\n",
    "                           'group1_num',\n",
    "                           'Accession']])\n",
    "    \"\"\" group1Data\n",
    "            group1_Intensity  group1_stdev  group1_num   Accession\n",
    "    0        2.824766e+05  1.708060e+05          15  A0A0B4J2D5\n",
    "    1        2.650998e+06  6.259645e+05          15      A2RUR9\n",
    "    2        1.973150e+05  5.645698e+04          15      A8MTJ3\n",
    "    3        2.524020e+05  1.355699e+05          15      A8MWD9\n",
    "    \"\"\"\n",
    "    group1Prots = group1Data.loc[:, ['Accession']]\n",
    "\n",
    "    group2Data = (Intensity_dict[group2]\n",
    "                  .assign(group2_Intensity=Intensity_dict[group2].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group2_stdev=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group2_num=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).shape[1]\n",
    "        - Intensity_dict[group2].isna().sum(axis=1))\n",
    "        .loc[:, ['group2_Intensity',\n",
    "                 'group2_stdev',\n",
    "                 'group2_num',\n",
    "                 'Accession']])\n",
    "    # find common proteins\n",
    "    commonProts = (group2Data.loc[:, ['Accession']]\n",
    "                   .merge(group1Prots, on='Accession', how='inner'))\n",
    "    # only leave common proteins\n",
    "    group2Data = (group2Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "    group1Data = (group1Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "\n",
    "    group2Median = group2Data['group2_Intensity'].median(\n",
    "        numeric_only=True)\n",
    "    group1Median = group1Data['group1_Intensity'].median(\n",
    "        numeric_only=True)\n",
    "    allmedian = (group2Data['group2_Intensity']\n",
    "                 .append(group1Data['group1_Intensity'])).median(\n",
    "        numeric_only=True)\n",
    "\n",
    "    # calculate the ratio between two group median,\n",
    "    # will be used to normalize them\n",
    "    if (Intensity_dict[group1].shape[1] > 3 and\n",
    "        Intensity_dict[group2].shape[1] > 3 and\n",
    "            group2 != group1):\n",
    "        ratio1 = allmedian / group1Median\n",
    "        ratio2 = allmedian / group2Median\n",
    "\n",
    "        # merge these two set of data together, adjust groups with ratio to \n",
    "        # median of all. Calculate pOriginal, p, significant\n",
    "        # pOriginal is a numpy array or list of p-values\n",
    "        # method is the method to be used for adjusting the p-values\n",
    "        volcanoData = (group2Data\n",
    "                       .merge(group1Data, on='Accession', how='inner'))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group1_Intensity=lambda x: volcanoData[\n",
    "                           'group1_Intensity'] * ratio1))\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group2_Intensity=lambda x: volcanoData[\n",
    "                           'group2_Intensity'] * ratio2))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(\n",
    "                           pOriginal=t_test_from_summary_stats(\n",
    "                               m1=volcanoData['group2_Intensity'],\n",
    "                               m2=volcanoData['group1_Intensity'],\n",
    "                               s1=volcanoData['group2_stdev'],\n",
    "                               s2=volcanoData['group1_stdev'],\n",
    "                               n1=volcanoData['group2_num'],\n",
    "                               n2=volcanoData['group1_num'])))\n",
    "        # filter out rows in volcanoData that have pOriginal == nan\n",
    "        # if pOriginal is nan, then the p value will be nan\n",
    "        volcanoData = volcanoData[volcanoData['pOriginal'].notna()]\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(p=multipletests(volcanoData[\n",
    "                           \"pOriginal\"], method='fdr_bh')[1]))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(significant=(abs(volcanoData[\n",
    "                           'group2_Intensity'] - volcanoData[\n",
    "                           'group1_Intensity']) > 1)\n",
    "                           & (volcanoData['p'] < 0.05)))\n",
    "\n",
    "        # add upRegulated, downRegulated, and notRegulated columns\n",
    "        volcanoData = volcanoData.assign(upRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"] - volcanoData[\n",
    "                \"group1_Intensity\"] > 1) & (volcanoData['significant']))\n",
    "\n",
    "        volcanoData = volcanoData.assign(downRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"] < -1) & (volcanoData['significant']))\n",
    "        volcanoData = volcanoData.assign(notRegulated=lambda x: (abs(\n",
    "           volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"]) <= 1) & (~volcanoData['significant']))\n",
    "        return plot_volcano_colored(\n",
    "            volcanoData,\n",
    "            label=f\"({group2}/{group1})\",\n",
    "            plot_options=plot_options,\n",
    "            username=username,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_volcano_colored(allData,\n",
    "                         label,\n",
    "                         plot_options=None,\n",
    "                         username=None,):\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "    total_labels = []\n",
    "    left = \"group1_Intensity\"\n",
    "    right = \"group2_Intensity\"\n",
    "    downData = allData[allData['downRegulated']\n",
    "                       == True]\n",
    "    upData = allData[allData['upRegulated'] == True]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        width=plot_options[\"width\"],\n",
    "        height=plot_options[\"height\"],)\n",
    "    if allData.shape[0] != 0:\n",
    "        fig.add_scatter(x=allData[right]-allData[left],\n",
    "                        y=-np.log10(allData[\"p\"]),\n",
    "                        text=allData[\"Accession\"],\n",
    "                        mode=\"markers\", marker=dict(\n",
    "                            color=plot_options[\"all color\"]))\n",
    "    if downData.shape[0] != 0:\n",
    "        fig.add_scatter(x=downData[right]-downData[left],\n",
    "                        y=-np.log10(downData[\"p\"]),\n",
    "                        text=downData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"down color\"]))\n",
    "    if upData.shape[0] != 0:\n",
    "        fig.add_scatter(x=upData[right]-upData[left],\n",
    "                        y=-np.log10(upData[\"p\"]),\n",
    "                        text=upData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"up color\"]))\n",
    "        fig.update_traces(\n",
    "            mode=\"markers\",\n",
    "            hovertemplate=\"%{text}<br>x=: %{x}\"\n",
    "            \" <br>y=: %{y}\")\n",
    "    fig.add_hline(y=-np.log10(0.05))\n",
    "    fig.add_vline(x=-1)\n",
    "    fig.add_vline(x=1)\n",
    "    if plot_options[\"title\"] != \"\" or plot_options[\"title\"] is not None:\n",
    "        plot_title = plot_options[\"title\"] + \" \" + label\n",
    "    else:\n",
    "        plot_title = None\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig.update_layout(\n",
    "        font=plot_options[\"font\"],\n",
    "\n",
    "        showlegend=False,\n",
    "        title=plot_title,\n",
    "        xaxis=dict(title=dict(\n",
    "            text=plot_options[\"X Title\"]), range=xlimits),\n",
    "        yaxis=dict(title=dict(\n",
    "            text=plot_options[\"Y Title\"]), range=ylimits),\n",
    "        annotations=total_labels\n",
    "\n",
    "    )\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_abundance_volcano_Plot.svg\"))\n",
    "        # create the download CSV and its link\n",
    "\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        allData.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_up_down_regulated_volcano.csv\"),\n",
    "            index=False)\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_up_down_regulated_volcano.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_abundance_volcano_Plot.svg\"\n",
    "    return fig, CSV_link, SVG_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###PCA plots####\n",
    "def PCA_plots(data_object, plot_options, saved_settings,username=None):\n",
    "    \"\"\"_Prepare data for creating intensity PCA plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    if not plot_options[\"compare groups\"] or \\\n",
    "            plot_options[\"compare groups\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"compare groups\"], list):\n",
    "        for item in GROUP_NAME[:2]:\n",
    "            if saved_settings[item]:\n",
    "                group_names.append(saved_settings[item])\n",
    "    else:  # compare groups (list of two, start with 0) is provided,\n",
    "        for n in range(len(GROUP_NAME)):\n",
    "            if n in plot_options[\"compare groups\"] and saved_settings[\n",
    "                    GROUP_NAME[n]]:\n",
    "                group_names.append(saved_settings[GROUP_NAME[n]])\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # this will contain list of run names list for each groups\n",
    "    #print(saved_settings)\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = [d.get('option')\n",
    "                           for d in saved_settings[f\"group{plot_options['compare groups'][i]+1}_record\"]]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1       \n",
    "        #print((runname_sublist))\n",
    "\n",
    "    all_runs =[item for sublist in runname_list for item in sublist]\n",
    "\n",
    "    # combined the data after filtering\n",
    "    #  missing values and log2 transformation/normalization\n",
    "    combined_infodata = pd.DataFrame() # store run names and group names\n",
    "    combined_pcaData = pd.DataFrame() # store normalized data and protein names\n",
    "    for eachGroup in group_names:\n",
    "\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        normalized_data = NormalizeToMedian(\n",
    "             current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "        toFileDict = dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],data_object[\"run_metadata\"][\"Run Names\"]))\n",
    "        toFileDict = generate_column_to_name_mapping(normalized_data.columns, toFileDict)\n",
    "        combined_infodata= pd.concat([combined_infodata, pd.DataFrame({\n",
    "            \"Sample_Groups\": normalized_data\n",
    "            .drop(\n",
    "                \"Accession\", axis=1).rename(columns = toFileDict).columns,\n",
    "            \"Type\": eachGroup})])\n",
    "        '''for x in range(len(list(combined_infodata[\"Sample_Groups\"]))):\n",
    "            print(list(combined_infodata[\"Sample_Groups\"])[x])\n",
    "        '''\n",
    "        if combined_pcaData.empty:\n",
    "            combined_pcaData = normalized_data\n",
    "        else:\n",
    "            combined_pcaData = pd.merge(combined_pcaData, normalized_data)\n",
    "\n",
    "    #normalize the data\n",
    "    # using ratio of current group median value divide by the all groups median \n",
    "    # to create a scaling factor magicNUm to scale the each group\n",
    "    for n in range(len(group_names)): \n",
    "        magicNum =np.nanmedian(combined_pcaData[runname_list[\n",
    "            n]].dropna(how='all').to_numpy()) /\\\n",
    "                np.nanmedian(combined_pcaData[\n",
    "            all_runs].dropna(how='all').to_numpy()) \n",
    "        for col in combined_pcaData[runname_list[\n",
    "                n]].columns:\n",
    "            combined_pcaData[col] = combined_pcaData[col]/magicNum\n",
    "\n",
    "\n",
    "    #performs k-Nearest Neighbors imputation to fill in any missing values\n",
    "    combined_pcaData = impute_knn(combined_pcaData)\n",
    "    combined_infodata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    # perform PCA transform\n",
    "    combined_pcaData, exp_var_pca = CalculatePCA(combined_pcaData,\n",
    "                                                     combined_infodata)\n",
    "\n",
    "    return plot_PCA_plotly(combined_pcaData,\n",
    "                           exp_var_pca,\n",
    "                           plot_options=plot_options,\n",
    "                           username=username,\n",
    "                           )\n",
    "\n",
    "\n",
    "def plot_PCA_plotly(pca_panda,\n",
    "                    exp_var_pca,\n",
    "                    plot_options=None,\n",
    "                    username=None,):\n",
    "\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    # Assuming pca_data is a pandas dataframe containing PCA results\n",
    "    # and \"Type\" is a column in the dataframe indicating the type of sample\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig = px.scatter(pca_panda,\n",
    "                     x='PC1',\n",
    "                     y='PC2',\n",
    "                     color=\"Type\",\n",
    "                     text=\"Sample_Groups\",\n",
    "                     symbol=\"Type\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "\n",
    "                     symbol_sequence=plot_options[\"symbol\"],\n",
    "                     size_max=30,\n",
    "                     labels={'PC1': f'PC1 ({round(exp_var_pca[0]*100,2)}%)',\n",
    "                             'PC2': f'PC2 ({round(exp_var_pca[1]*100,2)}%)',\n",
    "                             'Type': 'Sample Type'}, title='PCA Plot',\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],)\n",
    "\n",
    "    fig.update_traces(\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=plot_options[\"marker_size\"],),\n",
    "        hovertemplate=\"%{text}<br>PC1: %{x} <br>PC2: %{y}\")\n",
    "    fig.update_layout(\n",
    "        # plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n",
    "        # paper_bgcolor=\"rgba(0, 0, 0, 0)\",\n",
    "        font=plot_options[\"font\"],\n",
    "        title=plot_options[\"title\"],\n",
    "        xaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=xlimits),\n",
    "        yaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=ylimits),\n",
    "    )\n",
    "    if WRITE_OUTPUT:\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_PCA_Plot.svg\"))\n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        pca_panda.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_PCA.csv\"), index=False)\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_PCA.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_PCA_Plot.svg\"\n",
    "\n",
    "    return fig, CSV_link, SVG_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####All the functions and constants imports above############################\n",
    "### All the following sections are for configuration and plotting############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_This section read in data from the data management system or from the input\n",
    "files.\n",
    "Method 1 use process queue ID, serve address, account and password to read\n",
    "from data manage process queue_\n",
    "How to use: set process_queue_id and server_address, username and password\n",
    "\"\"\"\n",
    "\n",
    "process_queue_id =7272 # example MM PD3 6983, FP 7272\n",
    "server_address = \"10.37.240.41\"\n",
    "username = \"XiaofengXie\" #user name\n",
    "password = \"\" # DO NOT LEAVE your password when uploading to github\n",
    "if password != \"\" or process_queue_id == None:\n",
    "    queue_info, processor_info = queue_info_api(process_queue_id,\n",
    "                                                    server_address,\n",
    "                                                    username,\n",
    "                                                    password)\n",
    "    data_obj = read_file(queue_info=queue_info, processor_info=processor_info)\n",
    "else:\n",
    "    queue_info, processor_info = None, None\n",
    "\n",
    "\n",
    "# Method 2, use input file for customized tasks\n",
    "# read  custom data\n",
    "\"\"\"\n",
    "#FragePipe data\n",
    "data_obj = read_file(process_app = \"FragPipe\",\n",
    "    input1 = \"input/Fragpipe/combined_protein.tsv\",\n",
    "    input2 = \"input/Fragpipe/combined_peptide.tsv\",)\n",
    "data_obj = read_file(process_app = \"DIANN\",\n",
    "                     input1 = \"input/DIA/diann-output.pg_matrix.tsv\",\n",
    "                     input2 = \"input/DIA/diann-output.pr_matrix.tsv\",\n",
    "                     input3 = \"input/DIA/protein.tsv\",\n",
    "                     input4 = \"input/DIA/peptide.tsv\",\n",
    "                     input5 = \"input/DIA/filelist_diann.txt\",)\n",
    "#ximena pd3 data\n",
    "data_obj = read_file(process_app = \"PD\",\n",
    "                     input1 = \"input\\PD\\With10ngLibraries_Proteins.txt\",\n",
    "                     input2 = \"input\\PD\\With10ngLibraries_PeptideGroups.txt\",\n",
    "                     input3 = \"input/DIA/protein.tsv\",\n",
    "                     input4 = \"input/DIA/peptide.tsv\",\n",
    "                     input5 = \"input\\PD\\With10ngLibraries_InputFiles.txt\",)\n",
    "\n",
    "#MM PD3 data\n",
    "data_obj = read_file(process_app = \"PD\",\n",
    "                     input1 = \"input\\MM_PD3\\Full_MM_PD3_Proteins.txt\",\n",
    "                     input2 = \"input\\MM_PD3\\Full_MM_PD3_PeptideGroups.txt\",\n",
    "                     input5 = \"input\\MM_PD3\\Full_MM_PD3_InputFiles.txt\",)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\"\"\"\n",
    "#display(data_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a44dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [{\"input1\":\"C12-MS2-selected/combined_protein.tsv\",\n",
    "             \"input2\":\"C12-MS2-selected/combined_peptide.tsv\",\n",
    "             \"input3\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input4\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input5\":\"SingleCell_30m_MBR_KO_2_InputFiles.txt\",\n",
    "             \"process_app\": \"FragPipe\"},\n",
    "            {\"input1\":\"C12-50um-MS2-selected/combined_protein.tsv\",\n",
    "             \"input2\":\"C12-50um-MS2-selected/combined_peptide.tsv\",\n",
    "             \"input3\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input4\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input5\":\"SingleCell_30m_MBR_KO_2_InputFiles.txt\",\n",
    "             \"process_app\": \"FragPipe\"},\n",
    "            {\"input1\":\"C8-MS2-selected/combined_protein.tsv\",\n",
    "             \"input2\":\"C8-MS2-selected/combined_peptide.tsv\",\n",
    "             \"input3\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input4\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input5\":\"SingleCell_30m_MBR_KO_2_InputFiles.txt\",\n",
    "             \"process_app\": \"FragPipe\"},\n",
    "            ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group names and assign analysis to different groups(web app version was done through GUI so\n",
    "# steps are taken to make sure they have same output)\n",
    "x = read_files(grouped_input_files=filelist)\n",
    "data_obj = outer_join_data_objects(x)\n",
    "display(data_obj[\"run_metadata\"])\n",
    "#display(x[1][\"protein_abundance\"])\n",
    "#display(data_obj[\"protein_abundance\"])\n",
    "\n",
    "#display(data_obj)\n",
    "\n",
    "saved_settings = pd.read_table(SETTINGS_FILE,sep=\"\\t\").set_index(\"Group Name\").to_dict(orient=\"index\")\n",
    "#any run with any of the filter_out items will not be used.\n",
    "i = 0\n",
    "for eachGroup in saved_settings:\n",
    "    saved_settings[eachGroup][\"records\"] = []\n",
    "    filterOutType = type(saved_settings[eachGroup][\"filter_out\"])\n",
    "    if filterOutType == str or filterOutType == int or filterOutType == float and not pd.isna(saved_settings[eachGroup][\"filter_out\"]):\n",
    "        filterOut = str.split(saved_settings[eachGroup][\"filter_out\"],sep = \",\")\n",
    "    else:\n",
    "        filterOut = [\"M@di\"]\n",
    "    if len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) > 1:\n",
    "        user_list = []\n",
    "        #print(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[1:])\n",
    "        for each_fileID in str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[1:]:\n",
    "            for eachIdentifier in data_obj[\"run_metadata\"][\"Run Identifier\"]:    \n",
    "                currentRun = data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Identifier\"] == eachIdentifier][\"Run Names\"] \n",
    "                #print(eachIdentifier)\n",
    "                if currentRun.size == 1:\n",
    "                    if each_fileID == str.split(eachIdentifier,sep=\"-\")[0] and list(currentRun)[0] not in user_list:\n",
    "                        user_list.append(list(currentRun)[0])\n",
    "                        #print(eachIdentifier)\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(eachIdentifier + \"###\")\n",
    "        #print(user_list)\n",
    "        for run_name in user_list:\n",
    "            #display(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[0])\n",
    "            if str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[0] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                saved_settings[eachGroup][\"records\"].append(list(data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Names\"] == run_name][\"Run Names\"])[0]) \n",
    "            else:\n",
    "                pass\n",
    "               # print(run_name)\n",
    "            i = i + 1  \n",
    "    elif len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) == 1:\n",
    "            #print(\"all files\")\n",
    "            for run_name in data_obj[\"run_metadata\"][\"Run Names\"]:\n",
    "                if saved_settings[eachGroup][\"filter_in\"] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                    saved_settings[eachGroup][\"records\"].append(data_obj[\"run_metadata\"][\"Run Names\"][i])  \n",
    "                i = i + 1   \n",
    "#display(data_obj[\"run_metadata\"])\n",
    "display(saved_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID plot(protein and peptides)\n",
    "plot_options={\n",
    "            \"mean label\": \"True\",\n",
    "            \"error bar\": \"stdev\",\n",
    "            \"X Title\": \"Conditions\",\n",
    "            \"Y Title\": \"Protein Identification\",\n",
    "            \"color\": [\"blue\", \"red\", \"black\", \"yellow\", \"green\", \"purple\",\n",
    "                      \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ID mode\": \"total\",\n",
    "            \"help for information only\": \\\n",
    "            \"Mean label options: True or False.\" \\\n",
    "            \"error bar options: stdev or ci95.\" \\\n",
    "            \"color: the first few colors will be used\"\n",
    "            \"ID mode options: total, MS2, stacked.\" \\\n",
    "        }\n",
    "plot_options[\"plot_type\"] = \"1\" # 1 is protein, 2 is peptide\n",
    "figure ,_ ,_ =ID_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV violin plot\n",
    "plot_options={    \n",
    "        \"mean label\": \"True\",\n",
    "        \"box\": \"True\",\n",
    "        \"X Title\": \"Conditions\",\n",
    "        \"Y Title\": \"CV of Abundance (%)\",\n",
    "        \"color\": [\"blue\", \"red\", \"black\", \"yellow\", \"green\", \"purple\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "        \"width\": 700,\n",
    "        \"height\": 450,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"violinmode\": \"overlay\",\n",
    "        \"ylimits\": [0, 100],\n",
    "        \"help for information only\": \\\n",
    "        \"Mean label options: True or False.\" \\\n",
    "        \"color: the first few colors will be used\"\\\n",
    "        \"violinmode: group or overlay.\" \\\n",
    "    }\n",
    "figure, _, _ =CV_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID Venns plot\n",
    "plot_options={\n",
    "            \"compare groups\": [0,1],\n",
    "            \"title\": \"Venn Diagram\",\n",
    "            \"opacity\": 0.75,\n",
    "            \"color\": [\"#00FF00\", \"#FFFF00\", \"#FF0000\", \"yellow\", \"red\",\n",
    "                      \"green\", \"purple\", \"orange\", \"brown\", \"pink\",\n",
    "                      \"gray\",  \"olive\", \"cyan\", \"blue\",  \"black\", ],\n",
    "\n",
    "            \"help for information only\": \\\n",
    "            \"color: the first few colors will be used\"\n",
    "        }\n",
    "figure, _, _ =venns_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abundance Volcano plot\n",
    "plot_options={\n",
    "            \"compare groups\": [0, 1],\n",
    "            \"title\": \"Volcano Plot\",\n",
    "            \"X Title\": \"Log2 Fold Change\",\n",
    "            \"Y Title\": \"-Log10(p-value)\",\n",
    "            \"up color\": \"red\",\n",
    "            \"down color\": \"blue\",\n",
    "            \"all color\": \"gray\",\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ylimits\": [],\n",
    "            \"xlimits\": [],\n",
    "            \"help for information only\": \\\n",
    "            \"compare groups: list of two numbers default [0,1] means compare\" \\\n",
    "            \"the second group again the first group.\" \\\n",
    "            \"xlimits: list or tuple of two numbers [0,10].\" \\\n",
    "        }\n",
    "figure, _, _ =volcano_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9487aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abundance PCA plot\n",
    "plot_options={\n",
    "\n",
    "            \"compare groups\": [0,1],\n",
    "            \"title\": \"PCA Analysis\",\n",
    "            \"color\": [\"blue\", \"red\", \"black\", \"yellow\", \"green\", \"purple\",\n",
    "                      \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "            \"symbol\": ['star', 'circle'],\n",
    "            \"marker_size\": 8,\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ylimits\": [],\n",
    "            \"xlimits\": [],\n",
    "            \"help for information only\": \\\n",
    "            \"compare groups: list of two numbers default [0,1] means compare\" \\\n",
    "            \"color: the first few colors will be used\"\n",
    "            \"xlimits: list or tuple of two numbers [0,10].\" \\\n",
    "        }\n",
    "figure, _, _ =PCA_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a889046d7ab3fc532e7617178168e652077a93fddee421a252a8a2c494ffb595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
