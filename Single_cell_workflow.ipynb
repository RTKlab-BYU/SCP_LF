{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "import scipy\n",
    "import json\n",
    "import plotly.io as pio\n",
    "from plotly.offline import iplot\n",
    "import plotly as py\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import plotly.colors\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import scipy.stats\n",
    "import math\n",
    "from select import select\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "#only used for the app\n",
    "\n",
    "# import django\n",
    "# from django.conf import settings\n",
    "# from django.contrib.auth.decorators import login_required, permission_required\n",
    "# from file_manager.models import DataAnalysisQueue, SampleRecord, \\\n",
    "#     SavedVisualization, VisualizationApp, UserSettings, ProcessingApp\n",
    "# from django.shortcuts import render\n",
    "# from django.conf import settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants \n",
    "saved_settings ={}\n",
    "plot_options = {}\n",
    "JUPYTER_MODE = \"JPY_PARENT_PID\" in os.environ #check if it's in jupiter notebook mode\n",
    "APPFOLDER = \"./\"\n",
    "url_base = None\n",
    "\n",
    "#settings\n",
    "WRITE_OUTPUT = False\n",
    "USE_MaxLFQ = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is only for the webapp, not for jupyter notebook\n",
    "def get_run_name(queue_id):\n",
    "    \"\"\"_Get the run name/sample list from the result files, only\n",
    "    used for webapp for populate dropdown list_\n",
    "    Args:\n",
    "        queue_id (_int_): _task id from the process queue_\n",
    "    Returns:\n",
    "        _type_: _pandas data serial contains experiment list_\n",
    "        0              sample1\n",
    "        1              sample2\n",
    "        2              sample3\n",
    "    \"\"\"\n",
    "    if not queue_id:\n",
    "        return None\n",
    "    # get processing name\n",
    "    process_app = DataAnalysisQueue.objects.filter(\n",
    "        pk=queue_id).first().processing_app.name\n",
    "    # fragpipe results\n",
    "    if \"FragPipe\" in process_app:\n",
    "        peptide_file = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2\n",
    "        peptide = pd.read_table(peptide_file)\n",
    "        #\n",
    "        # get experiment names from columns names containning \" MaxLFQ Intensity\"\n",
    "        run_metadata = [\n",
    "            col for col in peptide.columns if \" MaxLFQ Intensity\" in col]\n",
    "        # remove \" MaxLFQ Intensity\" from the experiment names\n",
    "        run_metadata = [name.replace(\" MaxLFQ Intensity\", \"\")\n",
    "                            for name in run_metadata]\n",
    "        # create a pandas series to store the experiment names\n",
    "        run_metadata = pd.Series(run_metadata)\n",
    "    elif \"PD\" in process_app:\n",
    "        inpufile_6 = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_6\n",
    "        meta_table = pd.read_table(inpufile_6)\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        run_metadata =meta_table['file_names']\n",
    "\n",
    "    else:\n",
    "        run_metadata = pd.Series()\n",
    "\n",
    "    return run_metadata\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf24402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def queue_info_api(queue_id, server_address, user_name, password):\n",
    "    \"\"\"_Get the queue and app info from the server through API, this\n",
    "    is only for the jupyter notebook, not for the webapp_\n",
    "    \"\"\"\n",
    "\n",
    "    authinfo = HTTPBasicAuth(user_name, password)\n",
    "\n",
    "    #get queue info and test if the user name and password are correct   \n",
    "\n",
    "    queue_response = requests.get(\n",
    "        f'http://{server_address}/files/api/DataAnalysisQueue/{queue_id}/',\n",
    "        auth=authinfo\n",
    "    )\n",
    "    if queue_response.status_code != 200:\n",
    "        raise Exception(\"Invalid username or password\")\n",
    "    else:\n",
    "        queue_json_data = queue_response.json()\n",
    "\n",
    "    # Get app information\n",
    "    app_response = requests.get(\n",
    "        f\"http://{server_address}/files/api/ProcessingApp/{queue_json_data['processing_app']}/\",\n",
    "        auth=authinfo\n",
    "    )\n",
    "    app_json_data = app_response.json()\n",
    "    return queue_json_data, app_json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumIDs(IDMatrix):\n",
    "    \"\"\"_summarize the ID matrix infor into ID summary_\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        IDMatrix (_type_): _protein or pepetides matrix_\n",
    "        0 Accession/Annotated Sequence \trun1 \trun2 \trun3 \n",
    "        1 P023D12\tMS2 \tMBR \tNaN \n",
    "        2 P1222\tNaN \tID \tNaN \n",
    "    ID: means we don't know the ID mode\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "                                      names  MS2_IDs  MBR_IDs  Total_IDs\n",
    "0            10ng_QC_1_channel2 Intensity      NaN      NaN       3650\n",
    "1            10ng_QC_2_channel1 Intensity      NaN      NaN       3604\n",
    "....\n",
    "    \"\"\"\n",
    "    # Select the columns of the runs\n",
    "    columns = [col for col in IDMatrix.columns if not any(\n",
    "        substring in col for substring in [\n",
    "            'Accession', 'Annotated Sequence'])]\n",
    "    #put each ID_Modes into a list\n",
    "    returnNames = []\n",
    "    MS2_ID = []\n",
    "    MBR_ID = []\n",
    "    total_ID = []\n",
    "    for eachColumn in columns:\n",
    "        MS2_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MS2\"]))\n",
    "        MBR_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MBR\"]))\n",
    "        #print(IDMatrix[eachColumn])\n",
    "        total_ID_each = len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"ID\"])\n",
    "        if total_ID_each ==0:\n",
    "            total_ID_each = len(IDMatrix[eachColumn][\n",
    "                IDMatrix[eachColumn] == \"MS2\"]) + len(IDMatrix[\n",
    "                eachColumn][IDMatrix[eachColumn] == \"MBR\"])\n",
    "        total_ID.append(total_ID_each)\n",
    "\n",
    "    return pd.DataFrame({'names': columns,\n",
    "                         'MS2_IDs': MS2_ID,\n",
    "                         'MBR_IDs': MBR_ID,\n",
    "                         'Total_IDs': total_ID})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_from_name_mapping(columns, partial_column_name_mapping):\n",
    "    column_name_mapping = {}\n",
    "    #print(partial_column_name_mapping)\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key in col:\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_to_name_mapping(columns, partial_column_name_mapping):\n",
    "    column_name_mapping = {}\n",
    "    #print(partial_column_name_mapping)\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key == col:\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(queue_id=None, queue_info= None, processor_info = None,\n",
    "               input1=None, input2=None,input3=None, input4=None, input5=None,\n",
    "            process_app = None, file_id = 1):\n",
    "    \"\"\"_Read data from data manager API or through local files or read directly\n",
    "    in the webapp_\n",
    "    Args:\n",
    "        queue_id (_int_): _processing queue id_\n",
    "        queue_info (_dict_): _queue info from the API_\n",
    "        processor_info (_dict_): _processor info from the API_        \n",
    "        input1 (_str_): _input file 1_\n",
    "        input2 (_str_): _input file 2_\n",
    "        input3 (_str_): _input file 3_\n",
    "        input4 (_str_): _input file 4_\n",
    "        input5 (_str_): _input file 5_\n",
    "        process_app (_str_): _process app name_\n",
    "    Returns:\n",
    "        _dict_: _dictionary containing data all data        \n",
    "    \"\"\"\n",
    "\n",
    "    min_unique_peptides = 1\n",
    "\n",
    "    #getting files from data system\n",
    "        # getting files from data system (webapp)\n",
    "    if queue_id is not None and processor_info is None:\n",
    "        # Method 1 pull data directly (used by the webapp)\n",
    "        process_app = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().processing_app.name\n",
    "        input1= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_1\n",
    "        input2= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2  \n",
    "        input3= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_3\n",
    "        input4= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_4  \n",
    "        input5= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_5\n",
    "    elif queue_info is not None and processor_info is not None:\n",
    "    # Method 2 pull data from the data system (used by jupyter notebook)\n",
    "        process_app = processor_info[\"name\"]\n",
    "        input1= queue_info[\"output_file_1\"]\n",
    "        input2= queue_info[\"output_file_2\"]  \n",
    "        input3= queue_info[\"output_file_3\"]\n",
    "        input4= queue_info[\"output_file_4\"]  \n",
    "        input5= queue_info[\"output_file_5\"]\n",
    "    # method 3 feed data directly (through local file paths)\n",
    "    else:\n",
    "        analysis_file = input1\n",
    "\n",
    "    if \"FragPipe\" in process_app:     # fragpipe results\n",
    "        # read data\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "\n",
    "        # filter Contaminant\n",
    "        peptide_table= peptide_table[~peptide_table[\n",
    "            'Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        protein_table= protein_table[~protein_table['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)].query(\n",
    "            \"`Combined Total Peptides` >= @min_unique_peptides\")\n",
    "        \n",
    "        # get experiment names from columns names containning \"Intensity\"\n",
    "        # or \" MaxLFQ Intensity\" if MaxLFQ is used\n",
    "\n",
    "        if USE_MaxLFQ:\n",
    "            column_tail = \" MaxLFQ Intensity\"\n",
    "            intensity_cols = protein_table.columns[\n",
    "                protein_table.columns.str.contains(column_tail)].tolist()\n",
    "        else:\n",
    "            column_tail = \" Intensity\"\n",
    "            cols = protein_table.columns\n",
    "            intensity_cols =  protein_table.columns[( \\\n",
    "                cols.str.contains(\" Intensity\")) & \\\n",
    "                (~cols.str.contains(\" MaxLFQ Intensity\"))]\n",
    "        #get the column names of protein_table that contain \"Intensity\" but not \" MaxLFQ\"\n",
    "        \n",
    "        ## Proteins abundance table\n",
    "        protein_table.rename(columns={'Protein ID': 'Accession'},inplace=True)\n",
    "\n",
    "        all_path_cols = intensity_cols.append(pd.Index(['Accession']))\n",
    "\n",
    "        prot_abundance = protein_table.loc[:, all_path_cols]\n",
    "\n",
    "\n",
    "        ## Peptide abundance table\n",
    "        peptide_table.rename(columns={'Peptide Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "\n",
    "        all_path_cols = intensity_cols.append(pd.Index(['Annotated Sequence']))\n",
    "\n",
    "\n",
    "\n",
    "        pep_abundance = peptide_table.loc[:, all_path_cols]\n",
    "\n",
    "\n",
    "        # remove \" MaxLFQ Intensity\" or \" Intensity\" from names\n",
    "        run_name_list = [name.replace(column_tail, \"\")\n",
    "                            for name in intensity_cols]\n",
    "        \n",
    "        run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "        prot_abundance = prot_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              prot_abundance.columns if column_tail in col})\n",
    "\n",
    "        pep_abundance = pep_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              pep_abundance.columns if column_tail in col})\n",
    "\n",
    "        for item in [prot_abundance,pep_abundance]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            \n",
    "            fileid_mapping = generate_column_from_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        # get ID matrix tables\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Annotated Sequence\t']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        prot_other_info = protein_table.loc[\n",
    "            :, ~protein_table.columns.str.contains('Intensity')]\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info = peptide_table.loc[\n",
    "            :, ~peptide_table.columns.str.contains('Intensity')]\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "\n",
    "    elif \"DIANN\" in process_app:\n",
    "        # read in DIANN output files\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        prot_other_info = pd.read_table(input3,low_memory=False)\n",
    "        pep_other_info = pd.read_table(input4,low_memory=False)\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = input3\n",
    "        pep_other_info[\"Source_File\"] = input4\n",
    "\n",
    "        meta_table = pd.read_csv(input5, sep=' ', header=None, names=[\"File Name\"])\n",
    "        meta_table.reset_index(drop=True, inplace=True)\n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[~protein_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        peptide_table= peptide_table[~peptide_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        prot_other_info= prot_other_info[~prot_other_info['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        pep_other_info= pep_other_info[~pep_other_info['Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        \n",
    "        prot_other_info.rename(columns={'Protein': 'Accession'}, inplace=True)\n",
    "        pep_other_info.rename(columns={'Modified.Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "        # Replace backslashes with forward slashes if data comes from Windows\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['File Name'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        run_name_list = meta_table['File Name'].tolist()\n",
    "        run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "\n",
    "        # Get the file names from the meta table\n",
    "        protein_path_cols = protein_table.filter(regex='\\\\\\\\|Protein.Ids').columns\n",
    "\n",
    "        ## Proteins\n",
    "        prot_abundance = protein_table.loc[:, protein_path_cols]\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = protein_table.filter(regex='\\\\\\\\').columns\n",
    "        prot_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in prot_abundance.columns]\n",
    "        prot_abundance = prot_abundance.rename(columns={'Protein.Ids': 'Accession'})\n",
    "        for item in [prot_abundance,pep_abundance]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            fileid_mapping = generate_column_from_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        new_dict = {\"Found in Sample: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        #convert to str for IDs matrix\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        ## Peptides\n",
    "        peptide_path_cols = peptide_table.filter(regex='\\\\\\\\|Modified.Sequence').columns\n",
    "\n",
    "        pep_abundance = peptide_table.loc[:, peptide_path_cols]\n",
    "        pep_abundance = pep_abundance.rename(dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = peptide_table.filter(regex='\\\\\\\\').columns\n",
    "        pep_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in pep_abundance.columns]\n",
    "        pep_abundance = pep_abundance.rename(columns={'Modified.Sequence': 'Annotated Sequence'})\n",
    "\n",
    "        #convert to str for IDs matrix\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "     \n",
    "    elif \"PD\" in process_app:\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        \n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[(protein_table[\n",
    "            \"Protein FDR Confidence: Combined\"] == \"High\") &\n",
    "                        ((protein_table[\"Master\"] == \"IsMasterProtein\") | \n",
    "                         (protein_table[\"Master\"] == \"Master\")) & \n",
    "                        (protein_table[\"Contaminant\"] == False)]\n",
    "\n",
    "        protein_table.rename(\n",
    "            columns={'# Peptides': 'number of peptides'}, inplace=True)\n",
    "        protein_table=protein_table.query(\n",
    "            \"`number of peptides` >= @min_unique_peptides\")\n",
    "        peptide_table= peptide_table[(peptide_table[\n",
    "            'Contaminant'] == False) & (peptide_table[\"Confidence\"]== \"High\")]\n",
    "\n",
    "        meta_table = pd.read_table(input5,low_memory=False)\n",
    "        #filter rows in meta table on File ID column if it is NaN\n",
    "        meta_table = meta_table[meta_table['File ID'].notna()]\n",
    "\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        file_path_name_dict = dict(zip(meta_table['File ID'], meta_table['file_names']))\n",
    "        run_name_list = pd.DataFrame({\"Run Names\": file_path_name_dict.values()})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "        \n",
    "        #format the read in table into three different tables: abundance, id and other_info\n",
    "        prot_abundance = protein_table.filter(regex='Abundance:|Accession')\n",
    "        prot_ID = protein_table.filter(regex='Found in Sample:|Accession')\n",
    "        prot_other_info = protein_table.loc[:, ~protein_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "        \n",
    "\n",
    "        pep_abundance = peptide_table.filter(regex='Abundance:|Annotated Sequence')\n",
    "        pep_ID = peptide_table.filter(regex='Found in Sample:|Annotated Sequence')\n",
    "        pep_other_info = peptide_table.loc[:, ~peptide_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "        #change column names to file/run names to our fileID\n",
    "\n",
    "        new_dict = {\"Abundance: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [prot_abundance,pep_abundance]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "            #TODO solving  A value is trying to be set on a copy of a slice from a DataFrame\n",
    "            \n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            #use to name because we don't want partial matches\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        new_dict = {\"Found in Sample: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [pep_ID,prot_ID]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "\n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            #use to name because we don't want partial matches\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "\n",
    "        # replace \"High\" to MS2 \"Peak Found\" to MBR, the rest to NaN for the ID tables\n",
    "        replacements = {'High': 'MS2', 'Peak Found': 'MBR'}\n",
    "        for column in run_name_list[\"Run Identifier\"]:\n",
    "            if column in pep_ID.columns:\n",
    "                pep_ID[column] = pep_ID[column].replace(to_replace=replacements)\n",
    "    \n",
    "            if column in prot_ID.columns:\n",
    "                prot_ID[column] = prot_ID[column].replace(to_replace=replacements)\n",
    "    \n",
    "    #display(prot_ID)\n",
    "    # get ID summary, tolly\n",
    "    protein_ID_summary = sumIDs(prot_ID)\n",
    "    peptide_ID_summary = sumIDs(pep_ID)\n",
    "\n",
    "\n",
    "    \n",
    "#     peptide_intensities = AbundanceMatrix(\n",
    "#         peptide_table, maxLFQ_intensity=False, isProtein=False)\n",
    "#     peptide_ID_matrix = toIDMatrix(peptide_intensities)\n",
    "#     peptide_ID_summary = sumIDs(peptide_ID_matrix)\n",
    "\n",
    "    #sets the processing app in run_name_list\n",
    "    run_name_list[\"Processing App\"] = process_app\n",
    "    run_name_list[\"Analysis Name\"] = analysis_file\n",
    "\n",
    "\n",
    "    return {'run_metadata': run_name_list,\n",
    "            'protein_other_info': prot_other_info,\n",
    "            'peptide_other_info': pep_other_info,\n",
    "            'protein_abundance': prot_abundance,\n",
    "            'protein_ID_matrix': prot_ID,\n",
    "            'protein_ID_Summary': protein_ID_summary,\n",
    "            'peptide_abundance': pep_abundance,\n",
    "            'peptide_ID_matrix': pep_ID,\n",
    "            'peptide_ID_Summary': peptide_ID_summary,\n",
    "\n",
    "            }  \n",
    "\n",
    "# Define a custom replace function which is a stupid chatGPT suggestion, use pandas replace\n",
    "#def custom_replace(value):\n",
    "#    if value == 'High':\n",
    "#        return 'MS2'\n",
    "#    elif value == 'Peak Found':\n",
    "#        return 'MBR'\n",
    "#    else:\n",
    "#        return np.NaN\n",
    "\n",
    "# Function to generate a new column name mapping based on the partial_column_name_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(queue_ids = None, queue_info = None, processor_info = None, grouped_input_files = []):\n",
    "    '''\n",
    "    Creates a list of data objects\n",
    "\n",
    "    grouped_input_files\n",
    "    {input1:\n",
    "    input2:\n",
    "    input3:\n",
    "    input4:\n",
    "    input5:\n",
    "    process_app:\n",
    "    }\n",
    "    '''\n",
    "\n",
    "\n",
    "    data_objects = []\n",
    "\n",
    "    i = 0\n",
    "    for eachGroup in grouped_input_files:\n",
    "        if queue_ids is not None:\n",
    "            pass\n",
    "        else:\n",
    "            #print(eachGroup)\n",
    "            process_app = eachGroup[\"process_app\"]\n",
    "            input1= eachGroup[\"input1\"]\n",
    "            input2= eachGroup[\"input2\"]  \n",
    "            input3= eachGroup[\"input3\"]\n",
    "            input4= eachGroup[\"input4\"]  \n",
    "            input5= eachGroup[\"input5\"]\n",
    "\n",
    "        current_data_object = read_file(input1=input1,input2=input2,\n",
    "                                        input3=input3,input4 = input4,\n",
    "                                        input5=input5, process_app=process_app,file_id = i)\n",
    "        data_objects.append(current_data_object)\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    \n",
    "    return data_objects\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_join_data_objects(data_objects):\n",
    "    '''\n",
    "    Takes in a list of data objects as given by read_files and converts them to a single data object as given by read_files,\n",
    "    protein info continues to show what was found on each original file, and so forth.\n",
    "    '''\n",
    "\n",
    "    first_file = True\n",
    "    for eachDataObject in data_objects:\n",
    "        print(\"***\")\n",
    "        if first_file:\n",
    "            first_file = False\n",
    "            final_data_object = eachDataObject\n",
    "        else:\n",
    "            final_data_object['run_metadata'] = pd.concat([final_data_object['run_metadata'],eachDataObject['run_metadata']]).reset_index()\n",
    "            final_data_object['protein_other_info'] = pd.concat([final_data_object['protein_other_info'],eachDataObject['protein_other_info']]).reset_index()\n",
    "            final_data_object['peptide_other_info'] = pd.concat([final_data_object['peptide_other_info'],eachDataObject['peptide_other_info']]).reset_index()\n",
    "            final_data_object['protein_ID_Summary'] = pd.concat([final_data_object['protein_ID_Summary'],eachDataObject['protein_ID_Summary']]).reset_index()\n",
    "            final_data_object['peptide_ID_Summary'] = pd.concat([final_data_object['peptide_ID_Summary'],eachDataObject['peptide_ID_Summary']]).reset_index()\n",
    "            duplicates_found = False\n",
    "            \n",
    "            #loop through to see if there are any duplicate files\n",
    "            for eachCol in final_data_object['protein_abundance'].loc[:, final_data_object['protein_abundance'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['protein_ID_matrix'].loc[:, final_data_object['protein_ID_matrix'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_abundance'].loc[:, final_data_object['peptide_abundance'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_ID_matrix'].loc[:, final_data_object['peptide_ID_matrix'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass     \n",
    "            if duplicates_found:\n",
    "                print(\"Error: files analyzed twice present!!!\")\n",
    "                quit()\n",
    "                print(\"@#afio2q3\")\n",
    "            else:\n",
    "                #merge keeping all proteins\n",
    "                print(\"!!!!\")\n",
    "                final_data_object['protein_abundance'] = pd.merge(final_data_object['protein_abundance'],eachDataObject['protein_abundance'],how=\"outer\")\n",
    "                final_data_object['protein_ID_matrix'] = pd.merge(final_data_object['protein_ID_matrix'],eachDataObject['protein_ID_matrix'],how=\"outer\")\n",
    "                final_data_object['peptide_abundance'] = pd.merge(final_data_object['peptide_abundance'],eachDataObject['peptide_abundance'],how=\"outer\")\n",
    "                final_data_object['peptide_ID_matrix'] = pd.merge(final_data_object['peptide_ID_matrix'],eachDataObject['peptide_ID_matrix'],how=\"outer\")\n",
    "                \n",
    "    return final_data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_missing_values(data_object,\n",
    "                             missing_value_thresh=33,\n",
    "                             is_protein=True,\n",
    "                             ignore_nan=False):\n",
    "    \"\"\"_Filter out proteins/peptides with missing values rate above the\n",
    "    threshold_\n",
    "\n",
    "    Args:\n",
    "        data_object (_panada_): _dataframe contain data for one experimental\n",
    "        condition_\n",
    "        missing_value_thresh (int, optional): _description_. Defaults to 33.\n",
    "        analysis_program (str, optional): _description_.\n",
    "        ignore_nan: if filter intensity again with Nan threadshold, this \n",
    "        helps with the calcualting stdev step.\n",
    "\n",
    "    Returns:\n",
    "        _data_object_: _dictionary containing data for one experimental\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "0     A0A096LP49                            0.00\n",
    "1     A0A0B4J2D5                        89850.26\n",
    "2         A0AVT1                        83055.87\n",
    "    \"\"\"\n",
    "    if is_protein:\n",
    "        name = \"Accession\"\n",
    "        matrix_name = \"protein_ID_matrix\"\n",
    "        other_info_name = \"protein_other_info\"\n",
    "        abundance_name = \"protein_abundance\"\n",
    "        \n",
    "    else:\n",
    "        name = \"Annotated Sequence\"\n",
    "        matrix_name = \"peptide_ID_matrix\"\n",
    "        other_info_name = \"peptide_other_info\"\n",
    "        abundance_name = \"peptide_abundance\"\n",
    "\n",
    "    protein_columns = data_object[matrix_name].assign(missingValues=0)\n",
    "\n",
    "    i = 0\n",
    "    # found all the proteins/peptides with missing values rate below\n",
    "    # the threshold, pep_columns contains the remaining protein/peptide\n",
    "    # in a pandas dataframe with $names as its column name\n",
    "    for each_column in data_object[matrix_name].loc[\n",
    "            :, ~data_object[matrix_name].columns.str.contains(\n",
    "                name)].columns:\n",
    "        # replace \"nan\" to np.nan\n",
    "        protein_columns = protein_columns.replace({\"nan\": np.nan}) \n",
    "        #find missing values and increment\n",
    "\n",
    "        protein_columns.loc[protein_columns[each_column].isnull(),\n",
    "                             \"missingValues\"] += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    protein_columns = protein_columns.assign(missingValuesRate=(\n",
    "        protein_columns[\"missingValues\"] / i) * 100)\n",
    "    \n",
    "    protein_columns = protein_columns.query(\n",
    "        \"missingValuesRate < @missing_value_thresh\")\n",
    "    \n",
    "    protein_columns = protein_columns.loc[:,\n",
    "                                  protein_columns.columns.str.contains(name)]\n",
    "\n",
    "    # filter the data_object with the remaining proteins/peptides names\n",
    "    data_object[abundance_name] = protein_columns.merge(\n",
    "        data_object[abundance_name])\n",
    "    data_object[matrix_name] = protein_columns.merge(\n",
    "        data_object[matrix_name])\n",
    "    data_object[other_info_name] = protein_columns.merge(\n",
    "        data_object[other_info_name])\n",
    "    # In case there is mismatch between ID table and abundance table,\n",
    "    # mannually remove the row with all NaN values\n",
    "    # keep rows in data_object[abundance_name] where at least two values are \n",
    "    # not NaN(do this to all rows except the first row), otherwise can't\n",
    "    # calculate the stdev\n",
    "    if ignore_nan:\n",
    "        data_object[abundance_name] = data_object[abundance_name].dropna(\n",
    "            thresh=2, subset=data_object[abundance_name].columns[1:])\n",
    "        # This will cause the veen diagram to be different from R program\n",
    "    \n",
    "    return data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_missing_values_MS2(data_object,\n",
    "                             missing_value_thresh=33,\n",
    "                             is_protein=True,\n",
    "                             ignore_nan=False):\n",
    "    \"\"\"_Filter out proteins/peptides with missing values rate above the\n",
    "    threshold_\n",
    "\n",
    "    Args:\n",
    "        data_object (_panada_): _dataframe contain data for one experimental\n",
    "        condition_\n",
    "        missing_value_thresh (int, optional): _description_. Defaults to 33.\n",
    "        analysis_program (str, optional): _description_.\n",
    "        ignore_nan: if filter intensity again with Nan threadshold, this \n",
    "        helps with the calcualting stdev step.\n",
    "\n",
    "    Returns:\n",
    "        _data_object_: _dictionary containing data for one experimental\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "0     A0A096LP49                            0.00\n",
    "1     A0A0B4J2D5                        89850.26\n",
    "2         A0AVT1                        83055.87\n",
    "    \"\"\"\n",
    "    if is_protein:\n",
    "        name = \"Accession\"\n",
    "        matrix_name = \"protein_ID_matrix\"\n",
    "        other_info_name = \"protein_other_info\"\n",
    "        abundance_name = \"protein_abundance\"\n",
    "        \n",
    "    else:\n",
    "        name = \"Annotated Sequence\"\n",
    "        matrix_name = \"peptide_ID_matrix\"\n",
    "        other_info_name = \"peptide_other_info\"\n",
    "        abundance_name = \"peptide_abundance\"\n",
    "\n",
    "    protein_columns = data_object[matrix_name].assign(missingValues=0)\n",
    "\n",
    "    i = 0\n",
    "    # found all the proteins/peptides with missing values rate below\n",
    "    # the threshold, pep_columns contains the remaining protein/peptide\n",
    "    # in a pandas dataframe with $names as its column name\n",
    "    for each_column in data_object[matrix_name].loc[\n",
    "            :, ~data_object[matrix_name].columns.str.contains(\n",
    "                name)].columns:\n",
    "        # replace \"nan\" to np.nan\n",
    "        protein_columns = protein_columns.replace({\"nan\": np.nan}) \n",
    "        protein_columns.loc[protein_columns[each_column] != \"MS2\",\n",
    "                             \"missingValues\"] += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    protein_columns = protein_columns.assign(missingValuesRate=(\n",
    "        protein_columns[\"missingValues\"] / i) * 100)\n",
    "    \n",
    "    protein_columns = protein_columns.query(\n",
    "        \"missingValuesRate < @missing_value_thresh\")\n",
    "    \n",
    "    protein_columns = protein_columns.loc[:,\n",
    "                                  protein_columns.columns.str.contains(name)]\n",
    "\n",
    "    # filter the data_object with the remaining proteins/peptides names\n",
    "    data_object[abundance_name] = protein_columns.merge(\n",
    "        data_object[abundance_name])\n",
    "    data_object[matrix_name] = protein_columns.merge(\n",
    "        data_object[matrix_name])\n",
    "    data_object[other_info_name] = protein_columns.merge(\n",
    "        data_object[other_info_name])\n",
    "    # In case there is mismatch between ID table and abundance table,\n",
    "    # mannually remove the row with all NaN values\n",
    "    # keep rows in data_object[abundance_name] where at least two values are \n",
    "    # not NaN(do this to all rows except the first row), otherwise can't\n",
    "    # calculate the stdev\n",
    "    if ignore_nan:\n",
    "        data_object[abundance_name] = data_object[abundance_name].dropna(\n",
    "            thresh=2, subset=data_object[abundance_name].columns[1:])\n",
    "        # This will cause the veen diagram to be different from R program\n",
    "    \n",
    "    return data_object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeToMedian(abundance_data, apply_log2=False):\n",
    "    \"\"\"_Normalizes each column by multiplying each value in that column with\n",
    "    the median of all values in abundances (all experiments) and then dividing\n",
    "    by the median of that column (experiment)._\n",
    "    Args:\n",
    "        abundance_data (_pd_): _description_\n",
    "        apply_log2 (_bool_,): _apply log2 to all result_.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        format:\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "         A0A096LP49                    0.000000e+00\n",
    "    \"\"\"\n",
    "    # all the columns/sample list\n",
    "    columns = [col for col in abundance_data.select_dtypes(include=[\n",
    "            np.number])]\n",
    "    data_matrix = abundance_data[columns].values\n",
    "    # replace 0 with nan\n",
    "    data_matrix[data_matrix == 0] = np.nan\n",
    "    medianOfAll = np.nanmedian(data_matrix)\n",
    "    \n",
    "    #normalize all median, all median/current run all protein median\n",
    "    # apply log2 to all the values if apply_log2 is True\n",
    "    if apply_log2:    \n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                np.log2(medianOfAll) * np.log2(abundance_data[each_column]) /\n",
    "                np.log2(np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan))))\n",
    "    else:\n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                medianOfAll * abundance_data[each_column] /\n",
    "                np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan)))\n",
    "    #TODO divide by zero error encountered in log2, temporarily set to 0\n",
    "    abundance_data = abundance_data.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cvs(abundance_data):\n",
    "    \"\"\"_Calculate mean, stdev, cv for withn each protein/peptide abundance_\n",
    "\n",
    "    Args:\n",
    "        data_object (_type_): _full data frame_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _df with Accession mean, stdev, cv for each protein/peptide_\n",
    "    \"\"\"\n",
    "    if 'Accession' in abundance_data.columns:\n",
    "        name = \"Accession\"\n",
    "    if 'Annotated Sequence' in abundance_data.columns:\n",
    "        name = \"Annotated Sequence\"\n",
    "    abundance_data = abundance_data.assign(\n",
    "        intensity=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].mean(axis=1, skipna=True),\n",
    "        stdev=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].std(axis=1, skipna=True),\n",
    "        CV=abundance_data.loc[:, ~abundance_data.columns.str.contains(name)].std(\n",
    "            axis=1, skipna=True) / abundance_data.loc[\n",
    "            :, ~abundance_data.columns.str.contains(name)].mean(\n",
    "            axis=1, skipna=True) * 100)\n",
    "\n",
    "    abundance_data = abundance_data.loc[:, [\n",
    "            name, \"intensity\", \"stdev\", \"CV\"]]\n",
    "    \n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_from_summary_stats(m1, m2, n1, n2, s1, s2, equal_var=False):\n",
    "    \"\"\"_Calculate T-test from summary using ttest_ind_from_stats from\n",
    "    scipy.stats package_\n",
    "\n",
    "    Args:\n",
    "        m1 (_type_): _mean list of sample 1_\n",
    "        m2 (_type_): mean list of sample 2_\n",
    "        n1 (_type_): sample size list of sample 1_\n",
    "        n2 (_type_): sample size list of sample 2_\n",
    "        s1 (_type_): standard deviation list of sample 1_\n",
    "        s2 (_type_): standard deviation list of sample 2_\n",
    "        equal_var (_type_, optional): False would perform Welch's\n",
    "        t-test, while set it to True would perform Student's t-test. Defaults\n",
    "        to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _list of P values_\n",
    "    \"\"\"\n",
    "\n",
    "    p_values = []\n",
    "    for i in range(len(m1)):\n",
    "        _, p = ttest_ind_from_stats(\n",
    "            m1[i], s1[i], n1[i], m2[i], s2[i], n2[i], equal_var=equal_var)\n",
    "        p_values.append(p)\n",
    "\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineSharedInformation(infoObject1, infoObject2):\n",
    "    \"\"\"_Combine two infoObject into one_\n",
    "    #TODO: there must be a better way to achieve this.\n",
    "    \"\"\"\n",
    "\n",
    "    if infoObject1['meta'][\"quan_method\"] == 'Protein':\n",
    "        name = 'Accession'\n",
    "    else:\n",
    "        name = 'Annotated Sequence'\n",
    "\n",
    "    infoObject = {'meta': None,\n",
    "                  'run_name': None,\n",
    "                  'protein_data': None,\n",
    "                  'abundances': None,\n",
    "                  'protein_ID_matrix': None,\n",
    "                  'protein_ID_Summary': None}\n",
    "\n",
    "    if infoObject1['meta'] == infoObject2['meta']:\n",
    "        infoObject['meta'] = infoObject1['meta']\n",
    "        infoObject['run_name'] = pd.concat(\n",
    "            [infoObject1['run_name'], infoObject2['run_name']], axis=0)\n",
    "        infoObject['protein_ID_Summary'] = pd.concat(\n",
    "            [infoObject1['protein_ID_Summary'], infoObject2[\n",
    "                'protein_ID_Summary']], axis=0)\n",
    "        infoObject['protein_ID_matrix'] = pd.merge(\n",
    "            infoObject1['protein_ID_matrix'], infoObject2[\n",
    "                'protein_ID_matrix'], on=name)\n",
    "        infoObject = pd.merge(\n",
    "            infoObject1, infoObject2, on=name)\n",
    "        infoObject['protein_data'] = pd.merge(infoObject1[\n",
    "            'protein_data'].loc[:, infoObject1[\n",
    "                'protein_data'].columns.str.contains(name)],\n",
    "            infoObject2['protein_data'],\n",
    "            on=name)\n",
    "    else:\n",
    "        infoObject = \"ERROR: incompatible data types\"\n",
    "\n",
    "    return infoObject\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(abundance_data, k=5):\n",
    "    \"\"\"_inpute missing value from neighbor values_\n",
    "\n",
    "    Args:\n",
    "        abundance_data (_type_): _description_\n",
    "        k (int, optional): _number of neighbors used_. Defaults to 5.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        TODO: this knn imputer produces slightly different results (about 4%)\n",
    "        from the one in R. Need to figure out why\n",
    "    \"\"\"\n",
    "    name = abundance_data.columns[0]\n",
    "\n",
    "    names = abundance_data[name]\n",
    "    # x = abundance_data.select_dtypes(include=['float64', 'int64'])\n",
    "    # imputer = KNNImputer(n_neighbors=k)\n",
    "    # x_imputed = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)\n",
    "\n",
    "\n",
    "    x = abundance_data.select_dtypes(include=['float', 'int'])\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    x_imputed = imputer.fit_transform(x)\n",
    "    x_imputed = pd.DataFrame(x_imputed, columns=x.columns)\n",
    "\n",
    "\n",
    "\n",
    "    abundance_data.loc[:, x.columns] = x_imputed.values\n",
    "    abundance_data[name] = names\n",
    "    return abundance_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculatePCA(abundance_object, infotib,log2T = False):\n",
    "    \"\"\"_inpute PCA transformed and variance explained by each principal\n",
    "    component_\n",
    "    \"\"\"\n",
    "    name = abundance_object.columns[0]\n",
    "    x = abundance_object\n",
    "\n",
    "    sampleNames = x.columns[~x.columns.str.contains(\n",
    "        name)].to_frame(index=False)\n",
    "\n",
    "    if log2T: #apply log2 transformation\n",
    "        x = np.log2(x.loc[:, ~x.columns.str.contains(name)].T.values)\n",
    "    else:\n",
    "        x = x.loc[:, ~x.columns.str.contains(name)].T.values\n",
    "    # filter out columns with all zeros\n",
    "    is_finite_col = np.isfinite(np.sum(x, axis=0))\n",
    "    x_filtered = x[:, is_finite_col]\n",
    "\n",
    "    \n",
    "    # Instantiate PCA    \n",
    "    pca = PCA()\n",
    "    #\n",
    "    # Determine transformed features\n",
    "    #\n",
    "    x_pca = pca.fit_transform(x_filtered)\n",
    "    #\n",
    "    # Determine explained variance using explained_variance_ration_ attribute\n",
    "    #\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "    #\n",
    "    # Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "    # for visualizing the variance explained by each principal component.\n",
    "    #\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "    #\n",
    "    # convert numpy array to pandas dataframe for plotting\n",
    "    \n",
    "    pca_panda = pd.DataFrame(x_pca, columns=[\n",
    "        'PC' + str(i+1) for i in range(x_pca.shape[1])])\n",
    "    # add sample names to the dataframe\n",
    "    pca_panda = pd.concat(\n",
    "        [infotib, pca_panda], axis=1, join='inner')\n",
    "    \n",
    "    return pca_panda, exp_var_pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(data_dict, runname_list):\n",
    "    \"\"\"_Filter the data_dict based on runname_list, only keep the columns\n",
    "    of the data_dict that are in the runname_list_\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # make dict for each runname, no accession/sequence\n",
    "    nameDict = dict(zip(data_dict[\"run_metadata\"][\"Run Names\"],data_dict[\"run_metadata\"][\"Run Identifier\"]))\n",
    "    \n",
    "    identifier_list = []\n",
    "    \n",
    "    identifier_list_plus = []\n",
    "    if \"Annotated Sequence\" in runname_list:\n",
    "        runname_list.remove(\"Annotated Sequence\")\n",
    "    if \"Accession\" in runname_list:\n",
    "        runname_list.remove(\"Accession\")\n",
    "    for eachName in runname_list:\n",
    "        identifier_list.append(nameDict[eachName])\n",
    "\n",
    "    for eachName in runname_list:\n",
    "        identifier_list_plus.append(nameDict[eachName])\n",
    "\n",
    "\n",
    "    filtered_data = {}\n",
    "   # filtered_data[\"meta\"] = data_dict[\"meta\"]\n",
    "    runname_list.extend([\"Annotated Sequence\",\"Accession\"])\n",
    "    identifier_list_plus.extend([\"Annotated Sequence\",\"Accession\"])\n",
    "\n",
    "    #filtered_data[\"run_metadata\"] = [item for item in data_dict[\n",
    "    #   \"run_metadata\"] if item in runname_list]\n",
    "    \n",
    "    filtered_data[\"run_metadata\"] = data_dict[\"run_metadata\"][\n",
    "        data_dict[\"run_metadata\"][\"Run Names\"].isin(\n",
    "            runname_list)]  \n",
    "    filtered_data[\"protein_abundance\"] = data_dict[\"protein_abundance\"][[\n",
    "        col for col in data_dict[\"protein_abundance\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"peptide_abundance\"] = data_dict[\"peptide_abundance\"][[\n",
    "        col for col in data_dict[\"peptide_abundance\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_other_info\"] = data_dict[\"protein_other_info\"][[\n",
    "        col for col in data_dict[\"protein_other_info\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"peptide_other_info\"] = data_dict[\"peptide_other_info\"][[\n",
    "        col for col in data_dict[\"peptide_other_info\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_ID_matrix\"] = data_dict[\"protein_ID_matrix\"][[\n",
    "        col for col in data_dict[\"protein_ID_matrix\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_ID_Summary\"] = data_dict[\"protein_ID_Summary\"][\n",
    "        data_dict[\"protein_ID_Summary\"][\"names\"].isin(\n",
    "            identifier_list)]\n",
    "    filtered_data[\"peptide_ID_Summary\"] = data_dict[\"peptide_ID_Summary\"][\n",
    "        data_dict[\"peptide_ID_Summary\"][\"names\"].isin(\n",
    "            identifier_list)]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Volcano plots####\n",
    "def volcano_plots(data_object,  plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating intensity volcano plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    for each_key in saved_settings.keys():\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[each_key]:\n",
    "            group_names.append(each_key)\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "\n",
    "    for eachGroup in group_names:\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "            current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "    group1 = group_names[0]\n",
    "    group2 = group_names[1]\n",
    "    # calculate mean, standard deviation, and the number of non-null\n",
    "    # elements for each row/protein\n",
    "    group1Data = (Intensity_dict[group1]\n",
    "                  .assign(group1_Intensity=Intensity_dict[group1].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group1_stdev=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group1_num=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).shape[1] - Intensity_dict[\n",
    "                      group1].isna().sum(axis=1))\n",
    "                  .loc[:, ['group1_Intensity',\n",
    "                           'group1_stdev',\n",
    "                           'group1_num',\n",
    "                           'Accession']])\n",
    "    \"\"\" group1Data\n",
    "            group1_Intensity  group1_stdev  group1_num   Accession\n",
    "    0        2.824766e+05  1.708060e+05          15  A0A0B4J2D5\n",
    "    1        2.650998e+06  6.259645e+05          15      A2RUR9\n",
    "    2        1.973150e+05  5.645698e+04          15      A8MTJ3\n",
    "    3        2.524020e+05  1.355699e+05          15      A8MWD9\n",
    "    \"\"\"\n",
    "    group1Prots = group1Data.loc[:, ['Accession']]\n",
    "\n",
    "    group2Data = (Intensity_dict[group2]\n",
    "                  .assign(group2_Intensity=Intensity_dict[group2].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group2_stdev=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group2_num=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).shape[1]\n",
    "        - Intensity_dict[group2].isna().sum(axis=1))\n",
    "        .loc[:, ['group2_Intensity',\n",
    "                 'group2_stdev',\n",
    "                 'group2_num',\n",
    "                 'Accession']])\n",
    "    # find common proteins\n",
    "    commonProts = (group2Data.loc[:, ['Accession']]\n",
    "                   .merge(group1Prots, on='Accession', how='inner'))\n",
    "    # only leave common proteins\n",
    "    group2Data = (group2Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "    group1Data = (group1Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "\n",
    "    group2Median = group2Data['group2_Intensity'].median(\n",
    "        numeric_only=True)\n",
    "    group1Median = group1Data['group1_Intensity'].median(\n",
    "        numeric_only=True)\n",
    "    allmedian = (group2Data['group2_Intensity']\n",
    "                 .append(group1Data['group1_Intensity'])).median(\n",
    "        numeric_only=True)\n",
    "\n",
    "    # calculate the ratio between two group median,\n",
    "    # will be used to normalize them\n",
    "    if (Intensity_dict[group1].shape[1] > 3 and\n",
    "        Intensity_dict[group2].shape[1] > 3 and\n",
    "            group2 != group1):\n",
    "        ratio1 = allmedian / group1Median\n",
    "        ratio2 = allmedian / group2Median\n",
    "\n",
    "        # merge these two set of data together, adjust groups with ratio to \n",
    "        # median of all. Calculate pOriginal, p, significant\n",
    "        # pOriginal is a numpy array or list of p-values\n",
    "        # method is the method to be used for adjusting the p-values\n",
    "        volcanoData = (group2Data\n",
    "                       .merge(group1Data, on='Accession', how='inner'))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group1_Intensity=lambda x: volcanoData[\n",
    "                           'group1_Intensity'] * ratio1))\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group2_Intensity=lambda x: volcanoData[\n",
    "                           'group2_Intensity'] * ratio2))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(\n",
    "                           pOriginal=t_test_from_summary_stats(\n",
    "                               m1=volcanoData['group2_Intensity'],\n",
    "                               m2=volcanoData['group1_Intensity'],\n",
    "                               s1=volcanoData['group2_stdev'],\n",
    "                               s2=volcanoData['group1_stdev'],\n",
    "                               n1=volcanoData['group2_num'],\n",
    "                               n2=volcanoData['group1_num'])))\n",
    "        # filter out rows in volcanoData that have pOriginal == nan\n",
    "        # if pOriginal is nan, then the p value will be nan\n",
    "        volcanoData = volcanoData[volcanoData['pOriginal'].notna()]\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(p=multipletests(volcanoData[\n",
    "                           \"pOriginal\"], method='fdr_bh')[1]))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(significant=(abs(volcanoData[\n",
    "                           'group2_Intensity'] - volcanoData[\n",
    "                           'group1_Intensity']) > 1)\n",
    "                           & (volcanoData['p'] < 0.05)))\n",
    "\n",
    "        # add upRegulated, downRegulated, and notRegulated columns\n",
    "        volcanoData = volcanoData.assign(upRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"] - volcanoData[\n",
    "                \"group1_Intensity\"] > 1) & (volcanoData['significant']))\n",
    "\n",
    "        volcanoData = volcanoData.assign(downRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"] < -1) & (volcanoData['significant']))\n",
    "        volcanoData = volcanoData.assign(notRegulated=lambda x: (abs(\n",
    "           volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"]) <= 1) & (~volcanoData['significant']))\n",
    "        fig = plot_volcano_colored(\n",
    "            volcanoData,\n",
    "            label=f\"({group2}/{group1})\",\n",
    "            plot_options=plot_options,\n",
    "            username=username,\n",
    "        )\n",
    "        CSV_link = None\n",
    "        SVG_link = None\n",
    "        if WRITE_OUTPUT:\n",
    "            # create the file for donwnload\n",
    "            img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "            fig.write_image(os.path.join(\n",
    "                img_dir, f\"{username}_abundance_volcano_Plot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "            # create the download CSV and its link\n",
    "\n",
    "            data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "            if not os.path.exists(data_dir):\n",
    "                Path(data_dir).mkdir(parents=True)\n",
    "            volcanoData.to_csv(os.path.join(\n",
    "                data_dir, f\"{username}_up_down_regulated_volcano.csv\"),\n",
    "                index=False)\n",
    "            print(\"Downloading links...\")\n",
    "            CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "                f\"{username}_up_down_regulated_volcano.csv\"\n",
    "\n",
    "            # download SVG link\n",
    "            SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "                f\"{username}_abundance_volcano_Plot.svg\"\n",
    "        \n",
    "        return fig, CSV_link, SVG_link\n",
    "\n",
    "\n",
    "def plot_volcano_colored(allData,\n",
    "                         label,\n",
    "                         plot_options=None,\n",
    "                         username=None,):\n",
    "    \n",
    "    total_labels = []\n",
    "    left = \"group1_Intensity\"\n",
    "    right = \"group2_Intensity\"\n",
    "    downData = allData[allData['downRegulated']\n",
    "                       == True]\n",
    "    upData = allData[allData['upRegulated'] == True]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        width=plot_options[\"width\"],\n",
    "        height=plot_options[\"height\"],)\n",
    "    if allData.shape[0] != 0:\n",
    "        fig.add_scatter(x=allData[right]-allData[left],\n",
    "                        y=-np.log10(allData[\"p\"]),\n",
    "                        text=allData[\"Accession\"],\n",
    "                        mode=\"markers\", marker=dict(\n",
    "                            color=plot_options[\"all color\"]))\n",
    "    if downData.shape[0] != 0:\n",
    "        fig.add_scatter(x=downData[right]-downData[left],\n",
    "                        y=-np.log10(downData[\"p\"]),\n",
    "                        text=downData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"down color\"]))\n",
    "    if upData.shape[0] != 0:\n",
    "        fig.add_scatter(x=upData[right]-upData[left],\n",
    "                        y=-np.log10(upData[\"p\"]),\n",
    "                        text=upData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"up color\"]))\n",
    "        fig.update_traces(\n",
    "            mode=\"markers\",\n",
    "            hovertemplate=\"%{text}<br>x=: %{x}\"\n",
    "            \" <br>y=: %{y}\")\n",
    "    fig.add_hline(y=-np.log10(0.05))\n",
    "    fig.add_vline(x=-1)\n",
    "    fig.add_vline(x=1)\n",
    "    if plot_options[\"title\"] != \"\" or plot_options[\"title\"] is not None:\n",
    "        plot_title = plot_options[\"title\"] + \" \" + label\n",
    "    else:\n",
    "        plot_title = None\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig.update_layout(\n",
    "        font=plot_options[\"font\"],\n",
    "\n",
    "        showlegend=False,\n",
    "        title=plot_title,\n",
    "        xaxis=dict(title=dict(\n",
    "            text=plot_options[\"X Title\"]), range=xlimits),\n",
    "        yaxis=dict(title=dict(\n",
    "            text=plot_options[\"Y Title\"]), range=ylimits),\n",
    "        annotations=total_labels,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein peptide identification bar\n",
    "    plot_\n",
    "\n",
    "    Args:\n",
    "        data_dict (_type_): _description_\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to store the group names and filters\n",
    "    group_names = saved_settings.keys()\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    print(group_names)\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            runname_sublist)  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "        #print(group_dict[eachGroup][\"run_metadata\"])\n",
    "    #display(data_object[\"protein_ID_Summary\"])\n",
    "    #display(group_dict[eachGroup][\"protein_ID_Summary\"])\n",
    "    # create ID plots\n",
    "    # allIDs table will be used to store all experiment name, ID types (\n",
    "    # protein, peptide, MS2 and MS1 based), conditions and IDs numbers\n",
    "    allIDs = pd.DataFrame(\n",
    "        columns=[\"Names\", \"ID_Type\", \"ID_Mode\", \"Conditions\", \"IDs\"])\n",
    "\n",
    "    # loop through each group and extract IDs, put them into allIDs table\n",
    "    for eachCondition in group_names:\n",
    "        # Protein ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"protein_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"protein_ID_Summary\"].at[index, item]):\n",
    "                    # if the row with the item column is not empty,\n",
    "                    # add it to allIDs table.\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"protein\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "        # Peptide ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"peptide_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"peptide_ID_Summary\"].at[index, item]):\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"peptide\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "    # ######################allIDs format###################\n",
    "    # name\tID_Type\tID_Mode\tConditions\tIDs\n",
    "    # file1\tpeptide\tMS2_IDs\texperimetn 1\txxxxx\n",
    "    # file2\tprotein\tMBR_IDs\texperiment 2\txxxx\n",
    "    # file3\tpeptide\tTotal_IDs\texperiment 3\txxx\n",
    "    #######################################################\n",
    "    # Calcuate mean, standard deviation and number of replicates for each\n",
    "    export_ids = allIDs.copy()\n",
    "    # choose protein or peptide\n",
    "    if plot_options[\"plot_type\"] == \"1\":  # Protein ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"protein\"]\n",
    "    elif plot_options[\"plot_type\"] == \"2\":  # Peptide ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"peptide\"]\n",
    "\n",
    "    # choose total, MS2 or stacked\n",
    "    if plot_options[\"ID mode\"] == \"MS2\":  # MS2 ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"MS2_IDs\"]\n",
    "    elif plot_options[\"ID mode\"] == \"total\":\n",
    "        # total ID combined, if not already summed (key exist), sum them\n",
    "#         if allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"].empty:\n",
    "#             grouped = allIDs.groupby('name').agg(\n",
    "#                 {'IDs': 'sum', 'ID_Type': 'first', 'Conditions': 'first'})\n",
    "#             grouped = grouped.reset_index()\n",
    "#             grouped[\"ID_Mode\"] = \"Total_IDs\"\n",
    "#             allIDs = grouped\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "    elif plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "        pass\n",
    "    else:\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "    toPlotIDs = allIDs.groupby([\"ID_Mode\", \"Conditions\"]).agg({\n",
    "        'IDs': ['mean', 'std', 'count'], 'ID_Type': 'first', })\n",
    "\n",
    "    # rename the columns\n",
    "    toPlotIDs.columns = ['IDs', 'stdev', 'n', 'ID_Type']\n",
    "    # reset the index after grouping\n",
    "    toPlotIDs = toPlotIDs.reset_index()\n",
    "    # calculate the confidence interval based on 95%confidence interval`\n",
    "    toPlotIDs[\"confInt\"] = t.ppf(0.975, toPlotIDs['n']-1) * \\\n",
    "        toPlotIDs['stdev']/np.sqrt(toPlotIDs['n'])\n",
    "\n",
    "    #add columns for the categories specified in settings file (the one with all the filenames)\n",
    "    standard_groups = [\"filter_in\",\"filter_out\",\"records\"]\n",
    "    categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "    for eachCategory in categories:\n",
    "        toPlotIDs[eachCategory] = \"\"\n",
    "        for eachGroup in group_names:\n",
    "            toPlotIDs.loc[toPlotIDs[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "    \n",
    "    #display(toPlotIDs)\n",
    "    fig = plot_IDChart_plotly(toPlotIDs,\n",
    "                               username=username,\n",
    "                               plot_options=plot_options)\n",
    "\n",
    "    if WRITE_OUTPUT:    \n",
    "        # export the data to csv for user downloading\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        # create the directory if it does not exist\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "        \n",
    "        for eachCategory in categories:\n",
    "            export_ids[eachCategory] = \"\"\n",
    "            for eachGroup in group_names:\n",
    "                export_ids.loc[export_ids[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "        export_ids = export_ids.replace({\"Names\": dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],data_object[\"run_metadata\"][\"Run Names\"]))})\n",
    "        \n",
    "        # export the data to csv\n",
    "        export_ids.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_ID_data.csv\"), index=False)\n",
    "        \n",
    "        print(\"Downloading links...\")\n",
    "\n",
    "        # create the link for downloading the data\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_ID_data.csv\"\n",
    "\n",
    "        # add SVG download link\n",
    "\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_Bar_Plot.svg\"\n",
    "\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_Bar_Plot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        CSV_link = None\n",
    "        SVG_link = None\n",
    "    return fig, CSV_link, SVG_link\n",
    "\n",
    "def plot_IDChart_plotly(ID_data,\n",
    "                        username=None,\n",
    "                        plot_options=None):\n",
    "    \"\"\"_Plot the ID bar plot for the given data_\n",
    "\n",
    "    Args:\n",
    "        ID_data (_type_): _description_\n",
    "        username (str, optional): _description_. Defaults to \"test\".\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    plot_div = None\n",
    "    \n",
    "\n",
    "    if plot_options[\"ID mode\"] == \"grouped\":  \n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "        #find out present categories\n",
    "        categories = ID_data.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Bar(name = eachCategory,\n",
    "                        x=ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,\"IDs\"].tolist(),\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        text = round(ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,\"IDs\"].tolist()),\n",
    "                        error_y=dict(\n",
    "                            type = \"data\",\n",
    "                            array = ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,error_bars].tolist(),\n",
    "                            visible = error_visibile\n",
    "                        )))\n",
    "            i = i + 1                    \n",
    "\n",
    "        fig = go.Figure(data = fig_data,\n",
    "                        layout=go.Layout(yaxis_title=plot_options[\"Y Title\"],\n",
    "                        xaxis_title=plot_options[\"Group By X\"],\n",
    "                        barmode=\"group\",paper_bgcolor=\"rgba(255,255,255,255)\",\n",
    "                        plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                        yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                        xaxis=dict(showline=True, linewidth=1, linecolor='black')))\n",
    "        \n",
    "    elif plot_options[\"ID mode\"] == \"stacked\":\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "        if plot_options[\"Group By Stack\"] == \"ID_Mode\":\n",
    "            layers = [\"MS2_IDs\", \"MBR_IDs\"]\n",
    "        else:\n",
    "            layers = ID_data.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        fig_data = []\n",
    "        last_layer = None\n",
    "        i = 0\n",
    "        for eachLayer in layers: \n",
    "            if last_layer == None:\n",
    "                fig_data.append(go.Bar(\n",
    "                    name = eachLayer,\n",
    "                    x = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]].tolist(),\n",
    "                    y = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist(),\n",
    "                    marker_color = plot_options[\"color\"][i],\n",
    "                    text = round(ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()),\n",
    "                    error_y= dict(\n",
    "                        type = \"data\",\n",
    "                        array = ID_data.loc[ID_data[plot_options[\"Group By Stack\"]]==eachLayer,error_bars].tolist(),\n",
    "                        visible = error_visibile\n",
    "                    )\n",
    "                ))\n",
    "                i = i + 1\n",
    "            else:\n",
    "                fig_data.append(go.Bar(\n",
    "                    name = eachLayer,\n",
    "                    x = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]].tolist(),\n",
    "                    y = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist(),\n",
    "                    base=ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==last_layer),\"IDs\"].tolist(),\n",
    "                    marker_color = plot_options[\"color\"][i],\n",
    "                    opacity=0.5,\n",
    "                    text = round(ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()),\n",
    "                    error_y= dict(\n",
    "                        type = \"data\",\n",
    "                        array = ID_data.loc[ID_data[plot_options[\"Group By Stack\"]]==eachLayer,error_bars].tolist(),\n",
    "                        visible = error_visibile\n",
    "                    )\n",
    "                ))\n",
    "            last_layer = eachLayer\n",
    "        fig = go.Figure(\n",
    "                data = fig_data,\n",
    "                layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                barmode=\"stack\", \n",
    "                paper_bgcolor=\"rgba(255,255,255,255)\",\n",
    "                plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            ))                      \n",
    "    elif plot_options[\"ID mode\"] == \"grouped_stacked\":\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "        #make data tidy\n",
    "        if plot_options[\"Group By Stack\"] == \"ID_Mode\":\n",
    "            layers = [\"MS2_IDs\", \"MBR_IDs\"]\n",
    "        else:\n",
    "            layers = ID_data.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        categories = ID_data.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        \n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            last_layer = None\n",
    "            j = 0\n",
    "            for eachLayer in layers: \n",
    "                if last_layer == None:\n",
    "                    fig_data.append(go.Bar(\n",
    "                        name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                        x = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                        y = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"],\n",
    "                        offsetgroup=i,\n",
    "                        text = round(ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"]),\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        error_y = dict(\n",
    "                            type = \"data\",\n",
    "                            array = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),error_bars],\n",
    "                            visible=True)\n",
    "                    ))\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    fig_data.append(go.Bar(\n",
    "                        name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                        x = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                        y = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"],\n",
    "                        base=ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==last_layer),\"IDs\"],\n",
    "                        offsetgroup=i,\n",
    "                        text = round(ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"]),\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        opacity=1/2**j,\n",
    "                        error_y = dict(\n",
    "                            type = \"data\",\n",
    "                            array = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),error_bars],\n",
    "                            visible=True)\n",
    "                        ))\n",
    "                last_layer = eachLayer\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(\n",
    "            fig_data,\n",
    "            layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                barmode=\"group\",\n",
    "                plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "        else:\n",
    "            error_bars = None\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "        # create the plot\n",
    "        fig = px.bar(ID_data,\n",
    "                     x=\"Conditions\",\n",
    "                     y=\"IDs\",\n",
    "                     error_y=error_bars,\n",
    "                     color=\"Conditions\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],\n",
    "                     )\n",
    "        fig.update_layout(xaxis_title=plot_options[\"X Title\"],\n",
    "                          yaxis_title=plot_options[\"Y Title\"],\n",
    "                          annotations=total_labels,\n",
    "                          font=plot_options[\"font\"],\n",
    "                          plot_bgcolor =\"rgba(255, 255, 255, 255)\",\n",
    "                          paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                          yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                          xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "                          )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Violin plots ###\n",
    "def CV_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein CV violin plots_\n",
    "    \"\"\"\n",
    "    group_names = saved_settings.keys()\n",
    "\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "    if plot_options[\"Group By X\"] == \"ID_Mode\"or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "        Intensity_dict_MS2 = {}\n",
    "        for eachGroup in group_names:\n",
    "            current_condition_data = filter_by_missing_values(\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data[\"protein_abundance\"])\n",
    "            current_condition_data_MS2 = filter_by_missing_values_MS2( #returns\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict_MS2[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data_MS2[\"protein_abundance\"])\n",
    "    else:\n",
    "        for eachGroup in group_names:\n",
    "            current_condition_data = filter_by_missing_values(\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data[\"protein_abundance\"])\n",
    "\n",
    "    all_cvs = pd.DataFrame()\n",
    "\n",
    "    for eachGroup in Intensity_dict:\n",
    "        current = calculate_cvs(\n",
    "            Intensity_dict[eachGroup]).assign(Conditions=eachGroup,ID_Mode=\"All IDs\")\n",
    "        all_cvs = pd.concat([all_cvs, current], ignore_index=True)\n",
    "    if plot_options[\"Group By X\"] == \"ID_Mode\"or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "        print(\"MS2 CVs...\")\n",
    "        for eachGroup in Intensity_dict_MS2:\n",
    "            current = calculate_cvs(\n",
    "                Intensity_dict_MS2[eachGroup]).assign(Conditions=eachGroup,ID_Mode=\"MS2 IDs\")\n",
    "            all_cvs = pd.concat([all_cvs, current], ignore_index=True)\n",
    "\n",
    "    #add columns for the categories specified in settings file (the one with all the filenames)\n",
    "    standard_groups = [\"filter_in\",\"filter_out\",\"records\"]\n",
    "    categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "    for eachCategory in categories:\n",
    "        all_cvs[eachCategory] = \"\"\n",
    "        for eachGroup in group_names:\n",
    "            all_cvs.loc[all_cvs[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "\n",
    "\n",
    "    # ######################all_CVs format###################\n",
    "#      Accession     intensity          stdev          CV   Conditions\n",
    "# 0       A6NHR9  3.248547e+06  672989.819300   20.716643    DDMandDTT\n",
    "# 1       A8MTJ3  5.031539e+05  195535.383583   38.861944    DDMandDTT\n",
    "# 2       E9PAV3  5.330290e+05  161385.491163   30.277056    DDMandDT\n",
    "    #######################################################\n",
    "\n",
    "    return plot_CV_violin(allCVs=all_cvs,\n",
    "                          username=username,\n",
    "                          plot_options=plot_options)\n",
    "\n",
    "\n",
    "def plot_CV_violin(allCVs,\n",
    "                   username=None,\n",
    "                   plot_options=None,\n",
    "                   ):\n",
    "    \"\"\"_Plot the CV violin plot for the given data._\n",
    "\n",
    "    Args:\n",
    "        allCVs (_type_): _description_\n",
    "        username (_type_, optional): _description_. Defaults to None.\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    allCVs_summary = allCVs.groupby([\"Conditions\"]).agg(\n",
    "        {'CV': ['median', 'mean']}).reset_index()\n",
    "    allCVs_summary.columns = [\"Conditions\", 'meds', 'CoVar']\n",
    "    # mean label\n",
    "    if plot_options[\"mean label\"] == \"True\" or \\\n",
    "            plot_options[\"mean label\"] == True:\n",
    "        total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "            round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "            allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "    else:\n",
    "        total_labels = []   # no mean labels\n",
    "\n",
    "    if plot_options[\"CV mode\"] == \"grouped\":  \n",
    "            \n",
    "        #find out present categories\n",
    "        categories = allCVs.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Violin(name = eachCategory,\n",
    "                        x=allCVs.loc[allCVs[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=allCVs.loc[allCVs[plot_options[\"Group By Color\"]]==eachCategory,\"CV\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i]\n",
    "                        ))\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        \n",
    "        fig.update_layout(violinmode='group')\n",
    "    elif plot_options[\"CV mode\"] == \"stacked\":  \n",
    "            \n",
    "        #find out present categories\n",
    "        layers = allCVs.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "\n",
    "        for eachLayer in layers:\n",
    "            fig_data.append(go.Violin(name = eachLayer,\n",
    "                        x=allCVs.loc[allCVs[plot_options[\"Group By Stack\"]]==eachLayer,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=allCVs.loc[allCVs[plot_options[\"Group By Stack\"]]==eachLayer,\"CV\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i]\n",
    "                        ))\n",
    "            i = i + 1\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        \n",
    "        fig.update_layout(violinmode='overlay',\n",
    "                          plot_bgcolor='white',\n",
    "                          paper_bgcolor='white',\n",
    "                          yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                          xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "        )   \n",
    "    elif plot_options[\"CV mode\"] == \"grouped_stacked\":\n",
    "        #make data tidy\n",
    "        layers = allCVs.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        categories = allCVs.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for eachCategory in categories:\n",
    "            for eachLayer in layers:\n",
    "                fig_data.append(go.Violin(\n",
    "                    name = eachLayer + \" \" + eachCategory,\n",
    "                    x = allCVs.loc[(allCVs[plot_options[\"Group By Color\"]]==eachCategory)&(allCVs[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                    y = allCVs.loc[(allCVs[plot_options[\"Group By Color\"]]==eachCategory)&(allCVs[plot_options[\"Group By Stack\"]]==eachLayer),\"CV\"],\n",
    "                    offsetgroup=i,\n",
    "                    fillcolor = plot_options[\"color\"][j]\n",
    "                    ))\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(\n",
    "            fig_data,\n",
    "            layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                violinmode=\"group\",\n",
    "                plot_bgcolor='white',\n",
    "                paper_bgcolor='white',\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "    # create the interactive plot\n",
    "        fig = px.violin(allCVs,\n",
    "                        x=\"Conditions\",\n",
    "                        y='CV',\n",
    "                        color=\"Conditions\",\n",
    "                        box=bool(plot_options[\"box\"]),\n",
    "                        violinmode=plot_options[\"violinmode\"], hover_data=[\n",
    "                            \"Conditions\", 'CV'],\n",
    "                        color_discrete_sequence=plot_options[\"color\"],\n",
    "                        width=plot_options[\"width\"],\n",
    "                        height=plot_options[\"height\"],\n",
    "                        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                    range=plot_options[\"ylimits\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            font=plot_options[\"font\"],\n",
    "            xaxis=dict(title=plot_options[\"X Title\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            showlegend=True,\n",
    "            annotations=total_labels,\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "           \n",
    ")\n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_CV_Violin_Plot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        allCVs.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_CV.csv\"), index=False)\n",
    "        allCVs_summary.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_CV_summary.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_CV.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_CV_Violin_Plot.svg\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venns_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating ID veens plots (up to three groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    \n",
    "    for each_key in saved_settings.keys():\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[each_key]:\n",
    "            group_names.append(each_key)\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    data_set = []\n",
    "    labels_set = []\n",
    "    \n",
    "    for eachGroup in group_names:\n",
    "        \n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "\n",
    "        data_set.append(\n",
    "            set(current_condition_data['protein_abundance']['Accession'].unique()))\n",
    "        labels_set.append(eachGroup)\n",
    "\n",
    "    #print(data_set)\n",
    "\n",
    "    fig = venn_to_plotly(\n",
    "        data_set,\n",
    "        labels_set,\n",
    "        plot_options=plot_options,\n",
    "        username=username)\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        print(\"Downloading links...\")\n",
    "        # SVG file link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_venns_Plot.svg\"\n",
    "\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_venns_Plot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        \n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        i = 0\n",
    "        for eachSet in data_set:\n",
    "            pd.DataFrame({\"Accession\": list(eachSet)}).to_csv(os.path.join(\n",
    "                data_dir, f\"{username}_{labels_set[i]}_Venn.csv\"), index=False)\n",
    "            i = i + 1\n",
    "    \n",
    "    return fig, SVG_link, CSV_link\n",
    "\n",
    "def venn_to_plotly(L_sets,\n",
    "                   L_labels=None,\n",
    "                   plot_options=None,\n",
    "                   username=None):\n",
    "    \"\"\"_Creates a venn diagramm from a list of\n",
    "    sets and returns a plotly figure_\n",
    "    \"\"\"\n",
    "    \n",
    "    # get number of sets\n",
    "    n_sets = len(L_sets)\n",
    "\n",
    "    # choose and create matplotlib venn diagramm\n",
    "    if n_sets == 2:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn2(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn2(L_sets)\n",
    "    elif n_sets == 3:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn3(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn3(L_sets)\n",
    "    # supress output of venn diagramm\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Create empty lists to hold shapes and annotations\n",
    "    L_shapes = []\n",
    "    L_annotation = []\n",
    "\n",
    "    # Define color list for sets\n",
    "    L_color = plot_options[\"color\"]\n",
    "\n",
    "    # Create empty list to make hold of min and max values of set shapes\n",
    "    L_x_max = []\n",
    "    L_y_max = []\n",
    "    L_x_min = []\n",
    "    L_y_min = []\n",
    "\n",
    "    for i in range(0, n_sets):\n",
    "\n",
    "        # create circle shape for current set\n",
    "\n",
    "        shape = go.layout.Shape(\n",
    "            type=\"circle\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            x0=v.centers[i][0] - v.radii[i],\n",
    "            y0=v.centers[i][1] - v.radii[i],\n",
    "            x1=v.centers[i][0] + v.radii[i],\n",
    "            y1=v.centers[i][1] + v.radii[i],\n",
    "            fillcolor=L_color[i],\n",
    "            line_color=L_color[i],\n",
    "            opacity=plot_options[\"opacity\"]\n",
    "        )\n",
    "\n",
    "        L_shapes.append(shape)\n",
    "\n",
    "        # create set label for current set\n",
    "        try:\n",
    "            anno_set_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.set_labels[i].get_position()[0],\n",
    "                y=v.set_labels[i].get_position()[1],\n",
    "                text=v.set_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_set_label)\n",
    "\n",
    "            # get min and max values of current set shape\n",
    "            L_x_max.append(v.centers[i][0] + v.radii[i])\n",
    "            L_x_min.append(v.centers[i][0] - v.radii[i])\n",
    "            L_y_max.append(v.centers[i][1] + v.radii[i])\n",
    "            L_y_min.append(v.centers[i][1] - v.radii[i])\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "\n",
    "    # determine number of subsets\n",
    "    n_subsets = sum([scipy.special.binom(n_sets, i+1)\n",
    "                     for i in range(0, n_sets)])\n",
    "\n",
    "    for i in range(0, int(n_subsets)):\n",
    "        try:\n",
    "\n",
    "            # create subset label (number of common elements for current subset\n",
    "\n",
    "            anno_subset_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.subset_labels[i].get_position()[0],\n",
    "                y=v.subset_labels[i].get_position()[1],\n",
    "                text=v.subset_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_subset_label)\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "    # define off_set for the figure range\n",
    "    off_set = 0.2\n",
    "\n",
    "    # get min and max for x and y dimension to set the figure range\n",
    "    x_max = max(L_x_max) + off_set\n",
    "    x_min = min(L_x_min) - off_set\n",
    "    y_max = max(L_y_max) + off_set\n",
    "    y_min = min(L_y_min) - off_set\n",
    "\n",
    "    # create plotly figure\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # set xaxes range and hide ticks and ticklabels\n",
    "    fig.update_xaxes(\n",
    "        range=[x_min, x_max],\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set yaxes range and hide ticks and ticklabels\n",
    "    fig.update_yaxes(\n",
    "        range=[y_min, y_max],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set figure properties and add shapes and annotations\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(b=0, l=10, pad=0, r=10, t=40),\n",
    "        width=800,\n",
    "        height=400,\n",
    "        shapes=L_shapes,\n",
    "        annotations=L_annotation,\n",
    "        title=dict(text=plot_options[\"title\"], x=0.5, xanchor='center')\n",
    "    )\n",
    "    \n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###PCA plots####\n",
    "def PCA_plots(data_object, plot_options, saved_settings,username=None):\n",
    "    \"\"\"_Prepare data for creating intensity PCA plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    for each_key in saved_settings.keys():\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[each_key]:\n",
    "            group_names.append(each_key)\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # this will contain list of run names list for each groups\n",
    "    #print(saved_settings)\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1       \n",
    "        #print((runname_sublist))\n",
    "\n",
    "    all_runs =[item for sublist in runname_list for item in sublist]\n",
    "\n",
    "    # combined the data after filtering\n",
    "    #  missing values and log2 transformation/normalization\n",
    "    combined_infodata = pd.DataFrame() # store run names and group names\n",
    "    combined_pcaData = pd.DataFrame() # store normalized data and protein names\n",
    "    for eachGroup in group_names:\n",
    "\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        normalized_data = NormalizeToMedian(\n",
    "             current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "        toFileDict = dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],data_object[\"run_metadata\"][\"Run Names\"]))\n",
    "        toFileDict = generate_column_to_name_mapping(normalized_data.columns, toFileDict)\n",
    "        normalized_data.rename(columns = toFileDict,inplace=True)\n",
    "\n",
    "        combined_infodata= pd.concat([combined_infodata, pd.DataFrame({\n",
    "            \"Sample_Groups\": normalized_data\n",
    "            .drop(\n",
    "                \"Accession\", axis=1).rename(columns = toFileDict).columns,\n",
    "            \"Type\": eachGroup})])\n",
    "        \n",
    "        '''for x in range(len(list(combined_infodata[\"Sample_Groups\"]))):\n",
    "            print(list(combined_infodata[\"Sample_Groups\"])[x])\n",
    "        '''\n",
    "        if combined_pcaData.empty:\n",
    "            combined_pcaData = normalized_data\n",
    "            print(\"Empty\")\n",
    "        else:\n",
    "            combined_pcaData = pd.merge(combined_pcaData, normalized_data)\n",
    "\n",
    "    #normalize the data\n",
    "    # using ratio of current group median value divide by the all groups median \n",
    "    # to create a scaling factor magicNUm to scale the each group\n",
    "    quant_names = group_names\n",
    "    while \"Annotated Sequence\" in quant_names:\n",
    "        quant_names.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in quant_names:\n",
    "        quant_names.remove(\"Accession\")\n",
    "\n",
    "    while \"Annotated Sequence\" in all_runs:\n",
    "        all_runs.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in all_runs:\n",
    "        all_runs.remove(\"Accession\")\n",
    "\n",
    "    print(all_runs)\n",
    "    for n in range(len(quant_names)-2): #-2 because not Accession and Annotated Sequence \n",
    "        if \"Annotated Sequence\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Annotated Sequence\")\n",
    "        if \"Accession\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Accession\")\n",
    "        print(runname_list[n])\n",
    "        \n",
    "        magicNum =np.nanmedian(combined_pcaData[runname_list[\n",
    "            n]].dropna(how='all').to_numpy()) /\\\n",
    "                np.nanmedian(combined_pcaData[\n",
    "            all_runs].dropna(how='all').to_numpy()) \n",
    "        for col in combined_pcaData[runname_list[\n",
    "                n]].columns:\n",
    "            combined_pcaData[col] = combined_pcaData[col]/magicNum\n",
    "\n",
    "\n",
    "    #performs k-Nearest Neighbors imputation to fill in any missing values\n",
    "    combined_pcaData = impute_knn(combined_pcaData)\n",
    "    combined_infodata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    # perform PCA transform\n",
    "    combined_pcaData, exp_var_pca = CalculatePCA(combined_pcaData,\n",
    "                                                     combined_infodata)\n",
    "\n",
    "    return plot_PCA_plotly(combined_pcaData,\n",
    "                           exp_var_pca,\n",
    "                           plot_options=plot_options,\n",
    "                           username=username,\n",
    "                           )\n",
    "\n",
    "\n",
    "def plot_PCA_plotly(pca_panda,\n",
    "                    exp_var_pca,\n",
    "                    plot_options=None,\n",
    "                    username=None,):\n",
    "\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    # Assuming pca_data is a pandas dataframe containing PCA results\n",
    "    # and \"Type\" is a column in the dataframe indicating the type of sample\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig = px.scatter(pca_panda,\n",
    "                     x='PC1',\n",
    "                     y='PC2',\n",
    "                     color=\"Type\",\n",
    "                     text=\"Sample_Groups\",\n",
    "                     symbol=\"Type\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "\n",
    "                     symbol_sequence=plot_options[\"symbol\"],\n",
    "                     size_max=30,\n",
    "                     labels={'PC1': f'PC1 ({round(exp_var_pca[0]*100,2)}%)',\n",
    "                             'PC2': f'PC2 ({round(exp_var_pca[1]*100,2)}%)',\n",
    "                             'Type': 'Sample Type'}, title='PCA Plot',\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],)\n",
    "\n",
    "    fig.update_traces(\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=plot_options[\"marker_size\"],),\n",
    "        hovertemplate=\"%{text}<br>PC1: %{x} <br>PC2: %{y}\")\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "        paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "        font=plot_options[\"font\"],\n",
    "        title=plot_options[\"title\"],\n",
    "        xaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=xlimits),\n",
    "        yaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=ylimits),\n",
    "    )\n",
    "    if WRITE_OUTPUT:\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_PCA_Plot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        pca_panda.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_PCA.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_PCA.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_PCA_Plot.svg\"\n",
    "\n",
    "    return fig, CSV_link, SVG_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####All the functions and constants imports above############################\n",
    "### All the following sections are for configuration and plotting############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_This section read in data from the data management system or from the input\n",
    "files.\n",
    "Method 1 use process queue ID, serve address, account and password to read\n",
    "from data manage process queue_\n",
    "How to use: set process_queue_id and server_address, username and password\n",
    "\"\"\"\n",
    "\n",
    "process_queue_id =7272 # example MM PD3 6983, FP 7272\n",
    "server_address = \"10.37.240.41\"\n",
    "username = \"XiaofengXie\" #user name\n",
    "password = \"\" # DO NOT LEAVE your password when uploading to github\n",
    "if password != \"\" or process_queue_id == None:\n",
    "    queue_info, processor_info = queue_info_api(process_queue_id,\n",
    "                                                    server_address,\n",
    "                                                    username,\n",
    "                                                    password)\n",
    "    data_obj = read_file(queue_info=queue_info, processor_info=processor_info)\n",
    "else:\n",
    "    queue_info, processor_info = None, None\n",
    "\n",
    "\n",
    "# Method 2, use input file for customized tasks\n",
    "# read  custom data\n",
    "\"\"\"\n",
    "#FragePipe data\n",
    "data_obj = read_file(process_app = \"FragPipe\",\n",
    "    input1 = \"input/Fragpipe/combined_protein.tsv\",\n",
    "    input2 = \"input/Fragpipe/combined_peptide.tsv\",)\n",
    "data_obj = read_file(process_app = \"DIANN\",\n",
    "                     input1 = \"input/DIA/diann-output.pg_matrix.tsv\",\n",
    "                     input2 = \"input/DIA/diann-output.pr_matrix.tsv\",\n",
    "                     input3 = \"input/DIA/protein.tsv\",\n",
    "                     input4 = \"input/DIA/peptide.tsv\",\n",
    "                     input5 = \"input/DIA/filelist_diann.txt\",)\n",
    "#ximena pd3 data\n",
    "data_obj = read_file(process_app = \"PD\",\n",
    "                     input1 = \"input\\PD\\With10ngLibraries_Proteins.txt\",\n",
    "                     input2 = \"input\\PD\\With10ngLibraries_PeptideGroups.txt\",\n",
    "                     input3 = \"input/DIA/protein.tsv\",\n",
    "                     input4 = \"input/DIA/peptide.tsv\",\n",
    "                     input5 = \"input\\PD\\With10ngLibraries_InputFiles.txt\",)\n",
    "\n",
    "#MM PD3 data\n",
    "data_obj = read_file(process_app = \"PD\",\n",
    "                     input1 = \"input\\MM_PD3\\Full_MM_PD3_Proteins.txt\",\n",
    "                     input2 = \"input\\MM_PD3\\Full_MM_PD3_PeptideGroups.txt\",\n",
    "                     input5 = \"input\\MM_PD3\\Full_MM_PD3_InputFiles.txt\",)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\"\"\"\n",
    "#display(data_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a44dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put files in, one dictionary for each analysis\n",
    "filelist = [{\"input1\":\"Input\\May_2023\\FACS-H_vol_con\\Vol_con_6_9_Proteins.txt\",\n",
    "             \"input2\":\"Input\\May_2023\\FACS-H_vol_con\\Vol_con_6_9_PeptideGroups.txt\",\n",
    "             \"input3\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input4\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             \"input5\":\"Input\\May_2023\\FACS-H_vol_con\\Vol_con_6_9_InputFiles.txt\",\n",
    "             \"process_app\": \"PD\"},\n",
    "             #{\"input1\":\"SingleCell_30m_MBR_CTR_Proteins.txt\",\n",
    "             #\"input2\":\"SingleCell_30m_MBR_CTR_PeptideGroups.txt\",\n",
    "             #\"input3\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             #\"input4\":\"SingleCell_30m_MBR_KO_2_PeptideGroups.txt\",\n",
    "             #\"input5\":\"SingleCell_30m_MBR_CTR_InputFiles.txt\",\n",
    "             #\"process_app\": \"PD\"}\n",
    "            ]\n",
    "\n",
    "SETTINGS_FILE = \"settings_test_Ximena.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group names and assign analysis to different groups(web app version was done through GUI so\n",
    "# steps are taken to make sure they have same output)\n",
    "x = read_files(grouped_input_files=filelist)\n",
    "data_obj = outer_join_data_objects(x)\n",
    "#display(data_obj[\"run_metadata\"])\n",
    "#display(x[1][\"protein_abundance\"])\n",
    "#display(data_obj[\"protein_abundance\"])\n",
    "\n",
    "#display(data_obj)\n",
    "\n",
    "saved_settings = pd.read_table(SETTINGS_FILE,sep=\"\\t\").set_index(\"Group Name\").to_dict(orient=\"index\")\n",
    "#any run with any of the filter_out items will not be used.\n",
    "\n",
    "for eachGroup in saved_settings:\n",
    "    i = 0\n",
    "    saved_settings[eachGroup][\"records\"] = []\n",
    "    filterOutType = type(saved_settings[eachGroup][\"filter_out\"])\n",
    "    if filterOutType == str or filterOutType == int or filterOutType == float and not pd.isna(saved_settings[eachGroup][\"filter_out\"]):\n",
    "        filterOut = str.split(saved_settings[eachGroup][\"filter_out\"],sep = \",\")\n",
    "    else:\n",
    "        filterOut = [\"M@di\"]\n",
    "    if len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) > 1:\n",
    "        user_list = []\n",
    "        #print(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[1:])\n",
    "        for each_fileID in str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[1:]:\n",
    "            for eachIdentifier in data_obj[\"run_metadata\"][\"Run Identifier\"]:    \n",
    "                currentRun = data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Identifier\"] == eachIdentifier][\"Run Names\"] \n",
    "                #print(eachIdentifier)\n",
    "                if currentRun.size == 1:\n",
    "                    if each_fileID == str.split(eachIdentifier,sep=\"-\")[0] and list(currentRun)[0] not in user_list:\n",
    "                        user_list.append(list(currentRun)[0])\n",
    "                        #print(eachIdentifier)\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(eachIdentifier + \"###\")\n",
    "        #print(user_list)\n",
    "        for run_name in user_list:\n",
    "            #display(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[0])\n",
    "            if str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[0] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                saved_settings[eachGroup][\"records\"].append(list(data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Names\"] == run_name][\"Run Names\"])[0]) \n",
    "            else:\n",
    "                pass\n",
    "               # print(run_name)\n",
    "            i = i + 1  \n",
    "    elif len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) == 1:\n",
    "            #print(\"all files\")\n",
    "            for run_name in data_obj[\"run_metadata\"][\"Run Names\"]:\n",
    "                if saved_settings[eachGroup][\"filter_in\"] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                    saved_settings[eachGroup][\"records\"].append(data_obj[\"run_metadata\"][\"Run Names\"][i])  \n",
    "                i = i + 1   \n",
    "#display(data_obj[\"run_metadata\"])\n",
    "#display(data_obj[\"protein_ID_Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID plot(protein and peptides)\n",
    "plot_options={\n",
    "            \"mean label\": \"True\",\n",
    "            \"error bar\": \"ci95\",\n",
    "            \"X Title\": \"Conditions\",\n",
    "            \"Y Title\": \"Protein Identification\",\n",
    "            \"color\": [\"#3E6990\", \"#AABD8C\", \"#E9E3B4\", \"#F39B6D\", \"#C6878F\", \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"black\",\"red\",\n",
    "                  \"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"],\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ID mode\": \"grouped_stacked\",#grouped, total, MS2, grouped_stacked, stacked\n",
    "            \"Group By X\": \"Concentration\", #ID_Mode does MS2 vs MBR\n",
    "            \"Group By Color\": \"Volume\",#ID_Mode does MS2 vs MBR\n",
    "            \"Group By Stack\": \"ID_Mode\", #ID_Mode does MS2 vs MBR\n",
    "            \"help for information only\": \\\n",
    "            \"Mean label options: True or False.\" \\\n",
    "            \"error bar options: stdev or ci95.\" \\\n",
    "            \"color: the first few colors will be used\"\n",
    "            \"ID mode options: total, MS2, stacked, grouped.\" \\\n",
    "        }\n",
    "plot_options[\"plot_type\"] = \"1\" # 1 is protein, 2 is peptide\n",
    "figure ,_ ,_ =ID_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV violin plot\n",
    "plot_options={    \n",
    "        \"mean label\": \"True\",\n",
    "        \"box\": \"True\",\n",
    "        \"X Title\": \"Conditions\",\n",
    "        \"Y Title\": \"CV of Abundance (%)\",\n",
    "        \"color\": [\"#3E6990\", \"#AABD8C\", \"#E9E3B4\", \"#F39B6D\", \"#C6878F\", \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"black\",\"red\",\n",
    "                  \"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"],\n",
    "        \"width\": 700,\n",
    "        \"height\": 450,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"violinmode\": \"overlay\",\n",
    "        \"ylimits\": [0, 100],\n",
    "        \"CV mode\": \"stacked\",\n",
    "        \"Group By X\": \"Concentration\",\n",
    "        \"Group By Color\": \"Volume\",\n",
    "        \"Group By Stack\": \"ID_Mode\",\n",
    "        \"help for information only\": \\\n",
    "        \"Mean label options: True or False.\" \\\n",
    "        \"color: the first few colors will be used\"\\\n",
    "        \"violinmode: group or overlay.\" \\\n",
    "    }\n",
    "figure, _, _ =CV_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID Venns plot\n",
    "plot_options={\n",
    "            \"compare groups\": [\"A\",\"B\"],\n",
    "            \"title\": \"Venn Diagram\",\n",
    "            \"opacity\": 0.75,\n",
    "            \"color\": [\"#00FF00\", \"#FFFF00\", \"#FF0000\", \"yellow\", \"red\",\n",
    "                      \"green\", \"purple\", \"orange\", \"brown\", \"pink\",\n",
    "                      \"gray\",  \"olive\", \"cyan\", \"blue\",  \"black\", ],\n",
    "\n",
    "            \"help for information only\": \\\n",
    "            \"color: the first few colors will be used\"\n",
    "        }\n",
    "figure, _, _ =venns_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abundance Volcano plot\n",
    "plot_options={\n",
    "            \"compare groups\": [\"A\",\"B\"],\n",
    "            \"title\": \"Volcano Plot\",\n",
    "            \"X Title\": \"Log2 Fold Change\",\n",
    "            \"Y Title\": \"-Log10(p-value)\",\n",
    "            \"up color\": \"red\",\n",
    "            \"down color\": \"blue\",\n",
    "            \"all color\": \"gray\",\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ylimits\": [],\n",
    "            \"xlimits\": [],\n",
    "            \"help for information only\": \\\n",
    "            \"compare groups: list of two numbers default [0,1] means compare\" \\\n",
    "            \"the second group again the first group.\" \\\n",
    "            \"xlimits: list or tuple of two numbers [0,10].\" \\\n",
    "        }\n",
    "figure, _, _ =volcano_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9487aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abundance PCA plot\n",
    "plot_options={\n",
    "\n",
    "            \"compare groups\": [\"A\",\"B\"],\n",
    "            \"title\": \"PCA Analysis\",\n",
    "            \"color\": [\"blue\", \"red\", \"black\", \"yellow\", \"green\", \"purple\",\n",
    "                      \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "            \"symbol\": ['star', 'circle'],\n",
    "            \"marker_size\": 8,\n",
    "            \"width\": 700,\n",
    "            \"height\": 450,\n",
    "            \"font\": dict(size=16, family=\"Arial black\"),\n",
    "            \"ylimits\": [],\n",
    "            \"xlimits\": [],\n",
    "            \"help for information only\": \\\n",
    "            \"compare groups: list of two numbers default [0,1] means compare\" \\\n",
    "            \"color: the first few colors will be used\"\n",
    "            \"xlimits: list or tuple of two numbers [0,10].\" \\\n",
    "        }\n",
    "figure, _, _ =PCA_plots(data_obj, plot_options, saved_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efa007",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SECTION FOR PEAK PARAMETERS, MUST RUN APQUANT IN PD 2.5 #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea986d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_peaks_files(settings, data_files):\n",
    "    #load files\n",
    "    \n",
    "    files = []\n",
    "    for each_file in data_files:\n",
    "        currentFile = pd.read_table(each_file, sep=\"\\t\")\n",
    "        currentFile = currentFile[currentFile[\"Identified By\"] == \"MS/MS\"]\n",
    "        files.append(currentFile)\n",
    "    \n",
    "    #create groups\n",
    "    all_peaks = {}\n",
    "    in_filters = settings[\"filter_in\"].tolist()\n",
    "    out_filters = settings[\"filter_out\"].tolist()\n",
    "    i = 0\n",
    "    for eachGroup in settings[\"Group Name\"].tolist():\n",
    "        #check for which file\n",
    "        if len(str.split(in_filters[i],sep = \"@\")) == 2:\n",
    "            file_index = int(str.split(in_filters[i],sep = \"@\")[1])\n",
    "            filter_in = str.split(in_filters[i],sep = \"@\")[0]\n",
    "        elif len(str.split(in_filters[i],sep = \"@\")) == 1:\n",
    "            file_index = 0\n",
    "            filter_in = in_filters[i]\n",
    "        else:\n",
    "            print(\"error, multiple peak files per group not implemented\")\n",
    "            break\n",
    "        if isinstance(out_filters[i], str) and len(out_filters[i]) > 0:\n",
    "            filter_out = out_filters[i]\n",
    "            #print(filter_out)\n",
    "        elif out_filters[i] == None or out_filters[i] == \"\" or math.isnan(out_filters[i]):\n",
    "            #print(\"not \" + str(out_filters[i]))\n",
    "            filter_out = \"M@di\"\n",
    "        else:\n",
    "            filter_out = out_filters[i]\n",
    "            #print(filter_out)\n",
    "        #filter groups\n",
    "        currentGroupData = files[file_index][files[file_index][\"Spectrum File\"].str.contains(filter_in)]\n",
    "        currentGroupData = currentGroupData[~currentGroupData[\"Spectrum File\"].str.contains(filter_out)]\n",
    "        #group\n",
    "        all_peaks[eachGroup] = currentGroupData.groupby(\"Sequence\").agg({\n",
    "            \"Peak Apex\": \"median\",\n",
    "            \"FWHM\": \"mean\",\n",
    "            \"Apex Intensity\": \"mean\",\n",
    "            \"Area\": \"mean\"}).reset_index()\n",
    "        i = i + 1\n",
    "    return all_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FWHM_Boxplot (peakdata_object, plot_options, settings, username = None):\n",
    "\n",
    "    if len(settings[\"Group Name\"].tolist()) > 1:\n",
    "        firstName = settings[\"Group Name\"].tolist()[0]\n",
    "        all_fwhm = pd.DataFrame({\"Sequence\":peakdata_object[firstName][\"Sequence\"],firstName:peakdata_object[firstName][\"FWHM\"]})\n",
    "        for eachGroup in settings[\"Group Name\"].tolist()[1:]:\n",
    "            current = pd.DataFrame({eachGroup:peakdata_object[eachGroup][\"FWHM\"],\"Sequence\":peakdata_object[eachGroup][\"Sequence\"]})\n",
    "            all_fwhm = pd.merge(current, all_fwhm, how=\"inner\")\n",
    "    else:\n",
    "        all_fwhm = peakdata_object[0]\n",
    "        \n",
    "    toPlotFWHM = all_fwhm.set_index(\"Sequence\").melt(ignore_index=False, var_name=\"Group Name\", value_name=\"FWHM\").reset_index()\n",
    "    toPlotFWHM = toPlotFWHM.merge(settings, how=\"inner\")\n",
    "    return plot_FWHM_boxplot(toPlotFWHM, plot_options, username)\n",
    "\n",
    "def plot_FWHM_boxplot(FWHMs, plot_options, username):\n",
    "    \n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    if plot_options[\"log10\"]:\n",
    "        FWHMs= FWHMs[FWHMs[\"FWHM\"]>0]\n",
    "\n",
    "    #Remove outliers\n",
    "    if plot_options[\"outliers\"] == \"remove\":\n",
    "        q_low = FWHMs[\"FWHM\"].quantile(0.01)\n",
    "        q_hi  = FWHMs[\"FWHM\"].quantile(0.99)\n",
    "\n",
    "        FWHMs = FWHMs[(FWHMs[\"FWHM\"] < q_hi) & (FWHMs[\"FWHM\"] > q_low)]\n",
    "        plot_options[\"outliers\"] = False\n",
    "        \n",
    "    FWHM_summary = FWHMs.groupby([\"Group Name\"]).agg(\n",
    "        {'FWHM': ['median', 'mean']}).reset_index()\n",
    "    FWHM_summary.columns = [\"Group Name\", 'meds', 'CoVar']\n",
    "    # mean label\n",
    "    if plot_options[\"mean label\"] == \"True\" or \\\n",
    "            plot_options[\"mean label\"] == True:\n",
    "        total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "            round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "            FWHM_summary[\"Group Name\"], FWHM_summary[\"meds\"])]\n",
    "    else:\n",
    "        total_labels = []   # no mean labels\n",
    "\n",
    "    if plot_options[\"FWHM mode\"] == \"grouped\":  \n",
    "            \n",
    "        #find out present categories\n",
    "        categories = FWHMs.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Box(name = eachCategory,\n",
    "                        x=FWHMs.loc[FWHMs[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=FWHMs.loc[FWHMs[plot_options[\"Group By Color\"]]==eachCategory,\"FWHM\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i],\n",
    "                        boxpoints=plot_options[\"outliers\"],\n",
    "                        marker=dict(opacity=0)\n",
    "                        ))\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        fig.update_layout(\n",
    "            boxmode = \"group\",\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            xaxis=dict(title=plot_options[\"X Title\"],showline=True, linewidth=1, linecolor='black')\n",
    "            )\n",
    "        \n",
    "    else:\n",
    "    # create the interactive plot\n",
    "        fig = px.box(FWHMs,\n",
    "                        x=\"Group Name\",\n",
    "                        y='FWHM',\n",
    "                        color=\"Group Name\",\n",
    "                        color_discrete_sequence=plot_options[\"color\"],\n",
    "                        width=plot_options[\"width\"],\n",
    "                        height=plot_options[\"height\"],\n",
    "                        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                    range=plot_options[\"ylimits\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            font=plot_options[\"font\"],\n",
    "            xaxis=dict(title=plot_options[\"X Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            showlegend=True,\n",
    "            annotations=total_labels,\n",
    "            boxmode = \"group\",\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "        )\n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_FWHM_Boxplot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        FWHMs.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_FWHMs.csv\"), index=False)\n",
    "        FWHM_summary.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_FWHM_Summary.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_FWHMs.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_FWHM_boxplot.svg\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa75352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intensity_Boxplot (peakdata_object, plot_options, settings, username = None):\n",
    "\n",
    "    if len(settings[\"Group Name\"].tolist()) > 1:\n",
    "        firstName = settings[\"Group Name\"].tolist()[0]\n",
    "        all_intensity = pd.DataFrame({\"Sequence\":peakdata_object[firstName][\"Sequence\"],firstName:peakdata_object[firstName][\"Apex Intensity\"]})\n",
    "        for eachGroup in settings[\"Group Name\"].tolist()[1:]:\n",
    "            current = pd.DataFrame({eachGroup:peakdata_object[eachGroup][\"Apex Intensity\"],\"Sequence\":peakdata_object[eachGroup][\"Sequence\"]})\n",
    "            all_intensity = pd.merge(current, all_intensity, how=\"inner\")\n",
    "    else:\n",
    "        all_intensity = peakdata_object[0]\n",
    "        \n",
    "    toPlotIntensity = all_intensity.set_index(\"Sequence\").melt(ignore_index=False, var_name=\"Group Name\", value_name=\"Intensity\").reset_index()\n",
    "    toPlotIntensity = toPlotIntensity.merge(settings, how=\"inner\")\n",
    "    return plot_Intensity_boxplot(toPlotIntensity, plot_options, username)\n",
    "\n",
    "def plot_Intensity_boxplot(Intensities, plot_options, username):\n",
    "    \n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "\n",
    "    Intensities= Intensities[Intensities[\"Intensity\"]>0]\n",
    "\n",
    "    if plot_options[\"log10\"]:\n",
    "        Intensities[\"Intensity\"] = np.log10(Intensities[\"Intensity\"])\n",
    "\n",
    "    #Remove outliers\n",
    "    if plot_options[\"outliers\"] == \"remove\":\n",
    "        q_low = Intensities[\"Intensity\"].quantile(0.01)\n",
    "        q_hi  = Intensities[\"Intensity\"].quantile(0.99)\n",
    "\n",
    "        Intensities = Intensities[(Intensities[\"Intensity\"] < q_hi) & (Intensities[\"Intensity\"] > q_low)]\n",
    "        plot_options[\"outliers\"] = False\n",
    "        \n",
    "    Intensity_summary = Intensities.groupby([\"Group Name\"]).agg(\n",
    "        {'Intensity': ['median', 'mean']}).reset_index()\n",
    "    Intensity_summary.columns = [\"Group Name\", 'meds', 'CoVar']\n",
    "    # mean label\n",
    "    if plot_options[\"mean label\"] == \"True\" or \\\n",
    "            plot_options[\"mean label\"] == True:\n",
    "        total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "            round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "            Intensity_summary[\"Group Name\"], Intensity_summary[\"meds\"])]\n",
    "    else:\n",
    "        total_labels = []   # no mean labels\n",
    "\n",
    "    if plot_options[\"Intensity mode\"] == \"grouped\":  \n",
    "            \n",
    "        #find out present categories\n",
    "        categories = Intensities.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Box(name = eachCategory,\n",
    "                        x=Intensities.loc[Intensities[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=Intensities.loc[Intensities[plot_options[\"Group By Color\"]]==eachCategory,\"Intensity\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i],\n",
    "                        boxpoints=plot_options[\"outliers\"],\n",
    "                        marker=dict(opacity=0)\n",
    "                        ))\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        fig.update_layout(\n",
    "            boxmode = \"group\",\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            xaxis=dict(title=plot_options[\"X Title\"],showline=True, linewidth=1, linecolor='black'))\n",
    "        \n",
    "    else:\n",
    "    # create the interactive plot\n",
    "        fig = px.box(Intensities,\n",
    "                        x=\"Group Name\",\n",
    "                        y='Intensity',\n",
    "                        color=\"Group Name\",\n",
    "                        color_discrete_sequence=plot_options[\"color\"],\n",
    "                        width=plot_options[\"width\"],\n",
    "                        height=plot_options[\"height\"],\n",
    "                        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                    range=plot_options[\"ylimits\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            font=plot_options[\"font\"],\n",
    "            xaxis=dict(title=plot_options[\"X Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            showlegend=True,\n",
    "            annotations=total_labels,\n",
    "            boxmode = \"group\",\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "        )\n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_Intensity_Boxplot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        Intensities.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_Intensity.csv\"), index=False)\n",
    "        Intensity_summary.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_Intensity_Summary.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_Intensity.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_Intensity_boxplot.svg\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c44f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Area_Boxplot (peakdata_object, plot_options, settings, username = None):\n",
    "\n",
    "    if len(settings[\"Group Name\"].tolist()) > 1:\n",
    "        firstName = settings[\"Group Name\"].tolist()[0]\n",
    "        all_area = pd.DataFrame({\"Sequence\":peakdata_object[firstName][\"Sequence\"],firstName:peakdata_object[firstName][\"Area\"]})\n",
    "        for eachGroup in settings[\"Group Name\"].tolist()[1:]:\n",
    "            current = pd.DataFrame({eachGroup:peakdata_object[eachGroup][\"Area\"],\"Sequence\":peakdata_object[eachGroup][\"Sequence\"]})\n",
    "            all_area = pd.merge(current, all_area, how=\"inner\")\n",
    "    else:\n",
    "        all_area = peakdata_object[0]\n",
    "        \n",
    "    toPlotArea = all_area.set_index(\"Sequence\").melt(ignore_index=False, var_name=\"Group Name\", value_name=\"Area\").reset_index()\n",
    "    toPlotArea = toPlotArea.merge(settings, how=\"inner\")\n",
    "    return plot_Area_boxplot(toPlotArea, plot_options, username)\n",
    "\n",
    "def plot_Area_boxplot(Areas, plot_options, username):\n",
    "    \n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    SVG_link = None\n",
    "    #display(Areas)\n",
    "    Areas= Areas[Areas[\"Area\"]>0]\n",
    "    if plot_options[\"log10\"]:\n",
    "        Areas[\"Area\"] = np.log10(Areas[\"Area\"])\n",
    "    #display(Areas)\n",
    "\n",
    "    #Remove outliers\n",
    "    if plot_options[\"outliers\"] == \"remove\":\n",
    "        q_low = Areas[\"Area\"].quantile(0.01)\n",
    "        q_hi  = Areas[\"Area\"].quantile(0.99)\n",
    "\n",
    "        Areas = Areas[(Areas[\"Area\"] < q_hi) & (Areas[\"Area\"] > q_low)]\n",
    "        plot_options[\"outliers\"] = False\n",
    "    \n",
    "    Area_summary = Areas.groupby([\"Group Name\"]).agg(\n",
    "        {'Area': ['median', 'mean']}).reset_index()\n",
    "    Area_summary.columns = [\"Group Name\", 'meds', 'CoVar']\n",
    "    # mean label\n",
    "    if plot_options[\"mean label\"] == \"True\" or \\\n",
    "            plot_options[\"mean label\"] == True:\n",
    "        total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "            round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "            Area_summary[\"Group Name\"], Area_summary[\"meds\"])]\n",
    "    else:\n",
    "        total_labels = []   # no mean labels\n",
    "\n",
    "    if plot_options[\"Area mode\"] == \"grouped\":  \n",
    "            \n",
    "        #find out present categories\n",
    "        categories = Areas.groupby(plot_options[\"Group By Color\"]).first().reset_index()[plot_options[\"Group By Color\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Box(name = eachCategory,\n",
    "                        x=Areas.loc[Areas[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=Areas.loc[Areas[plot_options[\"Group By Color\"]]==eachCategory,\"Area\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i],\n",
    "                        boxpoints=plot_options[\"outliers\"],\n",
    "                        marker=dict(opacity=0)\n",
    "                        ))\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        fig.update_layout(\n",
    "            boxmode = \"group\", \n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            xaxis=dict(title=plot_options[\"X Title\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            legend=dict(itemclick=False, itemdoubleclick=False),\n",
    "            )\n",
    "        \n",
    "    else:\n",
    "    # create the interactive plot\n",
    "        fig = px.box(Areas,\n",
    "                        x=\"Group Name\",\n",
    "                        y='Area',\n",
    "                        color=\"Group Name\",\n",
    "                        color_discrete_sequence=plot_options[\"color\"],\n",
    "                        width=plot_options[\"width\"],\n",
    "                        height=plot_options[\"height\"],\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                    range=plot_options[\"ylimits\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            font=plot_options[\"font\"],\n",
    "            xaxis=dict(title=plot_options[\"X Title\"],showline=True, linewidth=1, linecolor='black'),\n",
    "            showlegend=True,\n",
    "            annotations=total_labels,\n",
    "            boxmode = \"group\",\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "        \n",
    "        )\n",
    "        \n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_Area_Boxplot.svg\"), format = \"svg\", validate = False, engine = \"kaleido\")\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        Areas.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_Areas.csv\"), index=False)\n",
    "        Area_summary.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_Area_Summary.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_Areas.csv\"\n",
    "\n",
    "        # download SVG link\n",
    "        SVG_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_Area_Violin_Plot.svg\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, SVG_link\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5398093",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEAKS_SETTINGS_FILE = \"peaks_settings.txt\"\n",
    "saved_peak_settings = pd.read_table(PEAKS_SETTINGS_FILE, sep=\"\\t\")\n",
    "PEAKS_DATA_FILES = [\"C12_apQuant_apQuantFeatures.txt\",\n",
    "                          \"50umC12_apQuantFeatures.txt\"]\n",
    "\n",
    "peakdata_obj = read_peaks_files(saved_peak_settings,PEAKS_DATA_FILES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fea464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FWHM boxplot\n",
    "display()\n",
    "plot_options={    \n",
    "        \"mean label\": \"True\",\n",
    "        \"X Title\": \"Conditions\",\n",
    "        \"Y Title\": \"FWHM (s)\",\n",
    "        \"color\": [\"blue\", \"cyan\", \"black\", \"gray\", \"purple\", \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "        \"log10\": False,\n",
    "        \"width\": 700,\n",
    "        \"height\": 450,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"outliers\": \"remove\", #False, \"all\", \"outliers\", \"suspectedoutliers\", \"remove\"\n",
    "        \"ylimits\": [0, 100],\n",
    "        \"FWHM mode\": \"ungrouped\", #grouped, ungrouped\n",
    "        \"Group By X\": \"Sample Amount\",\n",
    "        \"Group By Color\": \"SPE Type\",\n",
    "        \"help for information only\": \\\n",
    "        \"Mean label options: True or False.\" \\\n",
    "        \"color: the first few colors will be used\"\\\n",
    "    }\n",
    "figure, _, _ =FWHM_Boxplot(peakdata_obj, plot_options, saved_peak_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf51a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity boxplot\n",
    "display()\n",
    "plot_options={    \n",
    "        \"mean label\": \"True\",\n",
    "        \"X Title\": \"Conditions\",\n",
    "        \"Y Title\": \"Intensity\",\n",
    "        \"color\": [\"blue\", \"cyan\", \"black\", \"gray\", \"purple\", \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "        \"log10\": False,\n",
    "        \"width\": 700,\n",
    "        \"height\": 450,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"outliers\": \"all\", #False, \"all\", \"outliers\", \"suspectedoutliers\", \"remove\"\n",
    "        \"ylimits\": [0, 100],\n",
    "        \"Intensity mode\": \"grouped\", #grouped, ungrouped\n",
    "        \"Group By X\": \"Sample Amount\",\n",
    "        \"Group By Color\": \"SPE Type\",\n",
    "        \"help for information only\": \\\n",
    "        \"Mean label options: True or False.\" \\\n",
    "        \"color: the first few colors will be used\"\\\n",
    "    }\n",
    "figure, _, _ =Intensity_Boxplot(peakdata_obj, plot_options, saved_peak_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area boxplot\n",
    "display()\n",
    "plot_options={    \n",
    "        \"mean label\": \"True\",\n",
    "        \"X Title\": \"Conditions\",\n",
    "        \"Y Title\": \"Area\",\n",
    "        \"color\": [\"blue\", \"cyan\", \"black\", \"gray\", \"purple\", \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"],\n",
    "        \"width\": 700,\n",
    "        \"height\": 450,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"outliers\": \"remove\", #False, \"all\", \"outliers\", \"suspectedoutliers\", \"remove\"\n",
    "        \"log10\": False,\n",
    "        \"ylimits\": [0, 100],\n",
    "        \"Area mode\": \"grouped\", #grouped, ungrouped\n",
    "        \"Group By X\": \"Sample Amount\",\n",
    "        \"Group By Color\": \"SPE Type\",\n",
    "        \"help for information only\": \\\n",
    "        \"Mean label options: True or False.\" \\\n",
    "        \"color: the first few colors will be used\"\\\n",
    "    }\n",
    "figure, _, _ =Area_Boxplot(peakdata_obj, plot_options, saved_peak_settings)\n",
    "figure.show()\n",
    "#figure.write_image(\"images/test.svg\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a889046d7ab3fc532e7617178168e652077a93fddee421a252a8a2c494ffb595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
